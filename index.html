<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  

  
  <title>n3ird4</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="n3ird4">
<meta property="og:url" content="http://91.208.181.214/index.html">
<meta property="og:site_name" content="n3ird4">
<meta property="og:locale" content="fr">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="n3ird4">
  
    <link rel="alternate" href="/atom.xml" title="n3ird4" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">n3ird4</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Just a blog about SysOps H4c|&lt; and Petanque</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="Flux RSS"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Rechercher"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://91.208.181.214"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-AWS-SAAC-02" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/09/08/AWS-SAAC-02/" class="article-date">
  <time datetime="2020-09-08T19:21:42.000Z" itemprop="datePublished">2020-09-08</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/AWS/">AWS</a>►<a class="article-category-link" href="/categories/AWS/AWS-Certified-Solutions-Architect-Associate-SAA-C02/">AWS Certified Solutions Architect Associate SAA-C02</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/09/08/AWS-SAAC-02/">AWS-SAAC-02</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Yo this is my taking notes so far, work in progress</p>
<ol start="0">
<li>AWS - 10,000-Foot Overview</li>
<li>Identity and Access Management &amp; S3</li>
<li>EC2</li>
<li>Databases on AWS</li>
<li>Advanced IAM</li>
<li>Route 53</li>
<li>VPCs</li>
<li>HA Architecture</li>
<li>Applications</li>
<li>Security</li>
<li>Serverless</li>
<li>Good Luck with the Exam ;) </li>
</ol>
<h3 id="Lecture-AWS-10-000-Foot-Overview"><a href="#Lecture-AWS-10-000-Foot-Overview" class="headerlink" title="Lecture: AWS - 10,000-Foot Overview"></a>Lecture: AWS - 10,000-Foot Overview</h3><ul>
<li>A region is a physical location in the world which consists of two or more AZ.</li>
<li>An AZ is one or more discret data centers, each with redundant power, networking and connectivity, housed in separate facilities.</li>
<li>Edge Location are endpoints for AWS which are used for caching content. Typically this consists of CloudFront, Amazon’s Content Delivery Network (CDN)</li>
</ul>
<h1 id="Identity-and-Access-Management-amp-S3"><a href="#Identity-and-Access-Management-amp-S3" class="headerlink" title="Identity and Access Management &amp; S3"></a>Identity and Access Management &amp; S3</h1><ul>
<li>Buckets are universal name space and can’t share the same name.</li>
<li>Upload an objects to S3 receive a HTTP 200 code.</li>
<li>S3, S3-IA, S3-IA (One Zone), Glacier</li>
<li>Control access to buckets using either a bucket ACL or using Bucket Policies</li>
</ul>
<ul>
<li><a href="https://aws.amazon.com/s3/storage-classes/" target="_blank" rel="noopener">6 different Tiers</a>:</li>
</ul>
<p>S3 Standard<br>S3 Infrequent Access<br>S3 Intelligent - Tiering =&gt; exactly the same price except for the infrequent Acess there it becomes more advantageous<br>S3 One Zone - Infrequent Access<br>S3 Glacier<br>S3 Glacier Deep Archive</p>
<p>Bucket Policies =&gt; Bucket Level<br>Access Control Policy =&gt; Individual objects</p>
<p>S3 can be configured to create access logs.</p>
<p><strong>Encryption in TRANSIT is achieved by</strong>:</p>
<ul>
<li>SSL / TLS<br><strong>Encryption At REST (Server Side) is achieved by</strong></li>
<li>Service Side: (where AWS help you)<ul>
<li>SSE-S3 S3 Managed-Keys (ServerSideEncryption).</li>
<li>SSE-KMS AWS Key Management Service, Managed Keys.  </li>
<li>SSE-C Server Side Encryption with Customer provided key.</li>
</ul>
</li>
<li>Client Side:</li>
</ul>
<p><strong>Versioning</strong></p>
<ul>
<li>Stores all versions of an object (including all writes and even if you delete an object).</li>
<li>Great backup tool.</li>
<li>Once enabled, Versioning cannot be disabled, only suspended!</li>
<li>Integrate with lifecycle rules.</li>
<li>Versioning’s MFA delete capability (can be used to provide an additional layer of security).</li>
</ul>
<p><strong>LifeCycle Management with s3</strong></p>
<ul>
<li>Automates moving your objects between the different storage Tiers.</li>
<li>Can be used in conjunction with Versioning.</li>
<li>Can be applied to current versions and previous versions.</li>
</ul>
<p><strong>S3 Object Lock and Glacier Vault Lock</strong></p>
<ul>
<li>Use S3 ObjectLock to store objects using a write once read many (WORM) model.</li>
<li>Object Locks can be on Individual objects or applied across the bucket as a whole.</li>
<li>Object locks come in two modes:<ul>
<li>gouvernance mode : users can’t overwrite or delete an object version or alter its lock settings unless they have a special permissions.</li>
<li>compliance mode : a protected can’t be overwritten or deleted by any user, including the root user in your AWS account.</li>
</ul>
</li>
<li>S3 Glacier Vault Lock  :</li>
</ul>
<p><strong>S3 Performance</strong></p>
<ul>
<li>S3 Prefix : /folder1/folder.</li>
<li>S3 performance (3500 PUT) (5500GET) per second per prefix.</li>
<li>KMS Request Rates.</li>
<li><strong>S3 Multipart Uploads</strong>:<ul>
<li>Recommended for files over 100MB.</li>
<li>Required for files over 5GB.</li>
<li>Parallelize uploads (increase efficiency).</li>
</ul>
</li>
<li><strong>S3 Byte-Range-Fetches</strong>:<ul>
<li>Parallelize downloads by specifying byte ranges.</li>
<li>If there’s a failure in the download, it’s only for a specific byte range.</li>
</ul>
</li>
</ul>
<p><strong>S3 Select and Glacier Select</strong></p>
<ul>
<li>Is used to retrieve only a subset of data from an object by using simple SQL expressions.</li>
<li>Get data by rows or columns using simple SQL expressions.</li>
<li>Save money on data transfer and increase speed. (+400% !!) </li>
</ul>
<p><strong>AWS Organizations and Consolidated Billing</strong></p>
<ul>
<li>TODO once again…work in progress…</li>
<li>One bill per AWS account.</li>
<li>Very easy to track charges and allocate costs.</li>
<li>Volume pricing discount.</li>
</ul>
<p><strong>Sharing S3 Buckets Across Accounts</strong></p>
<ul>
<li>3 different ways to share S3 Buckets across accounts:<ul>
<li>Using bucket Policies &amp; IAM (applies across the entire bucket) Programmatic Access Only.</li>
<li>Using bucket ACL’s &amp; IAM (individual objects). Programmatic Access Only.</li>
<li>Cross-account IAM Roles. Programmatic AND Console access.</li>
</ul>
</li>
<li>TODO once again…work in progress…</li>
</ul>
<p><strong>Cross-Region Replication (CRR)</strong></p>
<ul>
<li>Versioning must be enabled on both the source and destination buckets.</li>
<li>Files in an existing bucket are not replicated automatically.</li>
<li>All subsequent updated files will be replicated automatically.</li>
<li>Delete markers are not replicated.</li>
<li>Deleting individual versions or delete markers will not be replicated.</li>
<li>Understand what Cross Region Replication is at a high level.</li>
</ul>
<p><em>Cross-region replication is an S3 feature where source and destination buckets are in different regions. Replication happens to objects uploaded/modified after the configuration is enabled. CRR allows for modification of the storage class or permissions on the destination object and can support objects encrypted with <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html" target="_blank" rel="noopener">SSE-S3</a> by default or <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html" target="_blank" rel="noopener">SSE-KMS</a> with additional configuration.</em></p>
<p><strong>S3 Transfer Acceleration</strong></p>
<p><em>S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket using Amazon CloudFront’s globally distributed edge locations.</em></p>
<ul>
<li><a href="http://s3-accelerate-speedtest.s3-accelerate.amazonaws.com/en/accelerate-speed-comparsion.html" target="_blank" rel="noopener">Speedtest s3</a></li>
</ul>
<p><strong>AWS DataSync</strong></p>
<ul>
<li>Use to move large amounts of data from on-premise to AWS.</li>
<li>Used with NFS and SMB compatible systems.</li>
<li>Replication can be done hourly, daily or weekly.</li>
<li>Install the Datasync agent to start the replication.</li>
<li>Can be used to replicate EFS to EFS.</li>
</ul>
<p><strong>CloudFront</strong></p>
<ul>
<li>CDN</li>
<li>Edge locations - This is the location where content will be cached. This is separate to an AWS Region/AZ.</li>
<li>Origin - This is the origin of all the files that the CDN will distribute. This can be an S3 Bucket, an EC2 Instance, an ELB or Route53.</li>
<li>Distribution - This is the name given to the CDN which consists of a collection of Edge Locations.</li>
<li>Web Distribution - Typically used for Websites.</li>
<li>RTMP - Used for Media Streaming.</li>
<li>Edge Location are not just READ only - you can write to them too. (ie put an object on to them.)</li>
<li>Objects are cached for the life of the TTL.</li>
<li>You can clear cached  objects, but you will be charged.</li>
</ul>
<p><strong>Create a CloudFront Distribution</strong></p>
<ul>
<li>Edge Location - This is the location where content will be cached. This is separate to an AWS Region/AZ</li>
<li>Origin - This is the origin of all the files that the CDN will distribute. This can be an S3 Bucket, an EC2 Instance, an ELB or Route53.</li>
<li>Distribution - This is the name given to the CDN which consists of a collection of Edge Locations.</li>
<li>Web Distribution - Typically used for Websites.</li>
<li>RTMP - Used for Media Streaming.</li>
<li>Edge Location are not just READ only - you can write to them too. (ie put an object on to them.)</li>
<li>Objects are cached for the life of the TTL.</li>
<li>You can clear cached  objects, but you will be charged.</li>
</ul>
<p><strong>CloudFront Signed URLs and Cookies</strong></p>
<ul>
<li>Use Signed URLs/Cookies when you want to secure content so that only the people you authorize are able to access it.</li>
<li>A Signed URL is for individual files. 1 file = 1 URL</li>
<li>A Signed Cookie is for Multiple files. 1 cookie = mutliple files</li>
<li>If your ORIGIN is EC2, then use CloudFront.</li>
<li>If your ORIGIN is s3, and single file then use S3 Signed url instead of a cloudfront signed url.</li>
</ul>
<p><strong>Snowball</strong></p>
<ul>
<li>Snowball can import to S3</li>
<li>Snowball can export from S3</li>
</ul>
<p><strong>Snowball Lab</strong></p>
<ul>
<li>Video cool à revoir mais pas utile pour l’exam.</li>
</ul>
<p><strong>Storage Gateway</strong></p>
<ul>
<li>File gateway<ul>
<li>File Gateway - For flat files, stored directly on S3</li>
</ul>
</li>
<li>Volume Gateway<ul>
<li>Stored Volumes - Entire Dataset is stored on site and is asynchronously backed up to S3.</li>
<li>Cached Volumes - Entire Dataset is stored on S3 and the most frequently accessed data is cached on site.</li>
</ul>
</li>
<li>Gateway Virtual Tape library</li>
</ul>
<p>File / Volume / Tape</p>
<p><strong>Athena vs. Macie [SAA-C02]</strong></p>
<p><strong>S3 and IAM Summary</strong></p>
<ul>
<li>IAM consists of the following:<ul>
<li>Users / Group / Roles / Policies</li>
</ul>
</li>
<li>IAM is universal. It does not apply to regions at this time.</li>
<li>the “root account” is simply the account created when first setup your AWS account. It has complete Admin access.</li>
<li>New Users have NO permissions when first created.</li>
<li>New Users are assigned Access Key ID &amp; Secret Access Keys when created.</li>
<li>These are not the same as a password. You cannot use the Access key ID &amp; Secret Access Key to login in to the console.<br>You can use this to access AWS via the APIs and Command Line, however.</li>
<li>You only get to view these once. if you loose them you have to regenerate them. So, save them in a secure location.</li>
</ul>
<p>=&gt; Always setup Multifactor Authentication on your root account.<br>=&gt; You can create and customise your own password rotation policies.</p>
<ul>
<li>Remember that S3 is OBJECT-BASED: i.e allows you to upload files.</li>
<li>Files can be from 0 Bytes to 5TB.</li>
<li>There is unlimited storage.</li>
<li>Files are stored in Buckets.</li>
<li><p>S3 is a universal namespace. That is, names must be unique globally.</p>
</li>
<li><p>Not suitable to install an operating system on.</p>
</li>
<li>Successful upload will generate a HTTP 200 status code.</li>
<li>By default all newly created buckets are PRIVATE. you can setup access control to your buckets using:<ul>
<li>Bucket Policies (bucket wide)</li>
<li>ACL (down to individual files or object)</li>
</ul>
</li>
</ul>
<p>The Key Fundamentals of S3 Are:</p>
<p>=&gt; Key (this is simply the name of the object)<br>=&gt; Value (This is simply data and is made up of a sequence of bytes)<br>=&gt; Version ID (important for versioning)<br>=&gt; Metadata (Data about data you are storing)<br>=&gt; Subresources (ACL, TOrrent)</p>
<ul>
<li>Read after Write consistency for PUTS of new objects.</li>
<li>Eventual consistency for overwrite PUTS and DELETES (can take some time to propagate)</li>
</ul>
<p>S3 Standard<br>S3 - IA<br>S3 Intelligent Tiering<br>S3 One Zone - IA<br>S3 Glacier<br>S3 Glacier Deep archive</p>
<p>Encryption in transit is achieved by:</p>
<ul>
<li>SSL/TLS<br>Encryption at REST (server side) is achieved by:</li>
<li>S3 Managed Keys - SSE-S3</li>
<li>AWS Key Management Service, Managed Keys - SSE-KMS</li>
<li>Server Side Encryption With Customer Provided Keys - SSE-C<br>Client Side Encryption</li>
</ul>
<p>Some Best Practices with AWS Organizations:</p>
<ul>
<li>Always setup Multifactor Authentication on root account.</li>
<li>Always use a strong and complex password on root account.</li>
<li>Paying account should be used for billing purpose only. Do not deploy resources into the paying account.</li>
<li>Enable / Disable AWS services using Service Control Policies (SCP) either on OU or on individual account.</li>
</ul>
<p>3 different ways to share S2 Buckets accross accounts:</p>
<ul>
<li>Using Bucket Policies &amp; IAM (applies across the entire bucket). Programmatic Acces only.</li>
<li>Using bucket ACLs &amp; IAM (individual objects). Programmatic access Only</li>
<li>Cross-account IAM Roles. Programmatic AND Console access.</li>
</ul>
<p>Cross Region Replication</p>
<ul>
<li>Versioning must be enabled on both the source and destination buckets.</li>
<li>Files in an existing bucket are not replicated automatically</li>
<li>All subsequent updated files will be replicated automatically.</li>
<li>Delete markers are not replicated.</li>
<li>Deleting individual versions or delete markers will not be replicated.</li>
<li>Understand what Cross Region Replication is at high level.</li>
</ul>
<p>LyceCycle policies</p>
<ul>
<li>Automates moving your objects between the different storage tiers.</li>
<li>Can be used in conjunction with versioning.</li>
<li>Can be applied to current versions and previons versions.</li>
</ul>
<p>S3 Transfert Acceleration</p>
<p>CloudFront</p>
<ul>
<li>Edge locations - This is the location where content will be cached. This is separate to an AWS Region/AZ.</li>
<li>Origin - This is the origin of all the files that the CDN will distribute. This can be an S3 Bucket, an EC2 Instance, an ELB or Route53.</li>
<li>Distribution - This is the name given to the CDN which consists of a collection of Edge Locations.</li>
<li>Web Distribution - Typically used for Websites.</li>
<li>RTMP - Used for Media Streaming.</li>
<li>Edge Location are not just READ only - you can write to them too. (ie put an object on to them.)</li>
<li>Objects are cached for the life of the TTL.</li>
<li>You can clear cached  objects, but you will be charged.</li>
</ul>
<p>Snowball</p>
<ul>
<li>Understand what Snowball is</li>
<li>import to S3</li>
<li>export from S3</li>
</ul>
<p>File gateway</p>
<ul>
<li>File Gateway - For flat files; stored directly on S3.</li>
<li>Volume Gateway<ul>
<li>stored Volumes - Entire Dataset is stored on site and is asynchronously backed up to S3.</li>
<li>Cached Volumes - Entire Dataset is stored on S3 and the most frequently accessed data is cached on site.</li>
</ul>
</li>
</ul>
<p>Gateway Virtual Tape library</p>
<ul>
<li><p>Used for backup and uses popular backup applications like netbackup, backup exec, veeam etc…</p>
<p>Athena Exam tips:</p>
<ul>
<li>Athena is an interactive query service</li>
<li>it allows you to query data located in S3 using standard SQL</li>
<li>Serverless</li>
<li>Commonly used to analyse log data stored in S3</li>
</ul>
</li>
</ul>
<p>Macie Exem tips:</p>
<ul>
<li>Macie uses AI to analyse data in S3 and helps indentify PII (personaly identifiable information)</li>
<li>Can also be used to analyse CloudTrail logs for suspicious API activity.</li>
<li>Includes Dashboards, Reports and Alerting</li>
<li>Great for PCI-DSS compliance and preventing ID theft.</li>
</ul>
<p>Read S3 FAQs before taking the exam. It comes up A LOT !</p>
<h1 id="Elastic-Compute-Cloud-EC2"><a href="#Elastic-Compute-Cloud-EC2" class="headerlink" title="Elastic Compute Cloud (EC2)"></a>Elastic Compute Cloud (EC2)</h1><p><strong>EC2 101 Elastic Compute Cloud</strong></p>
<ul>
<li>Termination protection is <em>turned off</em> by default, you must turn it on</li>
<li>On an EBS-backed instance, <em>the default action is for the root EBS volume to be deleted</em> when the instance is terminated.</li>
<li>EBS Root volumes of your DEFAULT AMI’s <em>CAN</em> be encryted. You can also use a third party tool (such as bit locker etc)<br>to encrypt the root volmume, or this can be done when creating AMI’s (lab to follow) in the AWS console or using API.</li>
<li>Additional volumes can be encrypted.</li>
</ul>
<p><strong>Security Groups - Lab</strong></p>
<ul>
<li>All inbounds traffic is blocked by default.</li>
<li>All outbounds traffic is allowed (statefull)</li>
<li>Changes to SG take effect immmediately.</li>
<li>You can have any number of EC2 instances within a security group.</li>
<li>You can have multiple SG attached to EC2 Instances.</li>
<li>SG are statefull</li>
<li>You can not block specific  IP addresses using SG , instead use NACL</li>
<li>You can specify allow rules, but not deny rules.</li>
</ul>
<p><strong>EBS 101</strong></p>
<ul>
<li>General Purpose SSD       =&gt; Most Work Loads      : gp2 -&gt; 1G-16T 16000 IOPS</li>
<li>Provisioned IOPS SSD      =&gt; Databases            : io1 -&gt; 4G-16T 64000 IOPS</li>
<li>Throughput Optimized HDD  =&gt; Big Data &amp; Warehouse : st1 -&gt; 500G-16T 500 IOPS</li>
<li>Cold HDD                  =&gt; File Servers         : sc1 -&gt; 500G-16T 250 IOPS</li>
<li>EBS Magnetic              =&gt; Workloads IAcess     : Standard -&gt; 1G-1T 40-200 IOPS</li>
</ul>
<p><strong>EBS Volumes and Snapshots</strong></p>
<ul>
<li>Volumes exist on EBS. Think of EBS as a virtual hard disk.</li>
<li>Snapshots exists on S3. Think of Snapshots as a photograp of the disk.</li>
<li>Snapshots are point in time copies of Volumes.</li>
<li>Snapshots are incremental.</li>
<li>First Snapshots may take some time to create</li>
<li>To create a Snapshots for EBS that serve as root devices, you should stop the instance before taking the snapshot.</li>
<li>However you can take a snap while the instance is running.</li>
<li>You can create AMI’s from snapshot</li>
<li>You can change EBS volume sizes on the fly, including changing the size and storage type.</li>
<li><p>Volumes will ALWAYS be in the same AZ as the EC2 instance.</p>
</li>
<li><p>To move an EC2 volume from one AZ to another, take as snapshot of it, crate an AMI from the snapshot and then use the AMI to launch the EC2 instance in a new AZ.</p>
</li>
<li>To move an EC2 volume from one REGION to another, take as snapshot of it, crate an AMI from the snapshot and then copy the AMI from one region to the other. Then use the copied AMI to launch the new EC2 instance in the new region.</li>
</ul>
<p><strong>AMI Types (EBS vs. Instance Store)</strong></p>
<ul>
<li>All AMIs are categorized as either backed by EBS or backed by instance store.<ul>
<li>The former means the root device for an instance launched from the AMI is an EBS volume created from an EBS snapshot.</li>
<li>The latter means the root device for an instance launched from the AMI is an instance store volume created from a template stored in Amazon S3.</li>
</ul>
</li>
<li>Instance Store Volumes are sometimes called Ephemeral Storage.</li>
<li>Instance Store Volumes cannot be stopped. If the underlying host fails, you will lose your data.</li>
<li>EBS backed instances can be stopped. You will not lose the data on this instance if it is stopped.</li>
<li>You can reboot both, you will not lose your data.</li>
<li>By Default, both ROOT volumes will be deleted on termination. However, with EBS volumes, you can tell AWS to keep the root device volume.</li>
</ul>
<p><strong>ENI vs. ENA vs. EFA [SAA-C02]</strong></p>
<p>-</p>
<p><strong>Encrypted Root Device Volumes and Snapshots - Lab</strong></p>
<ul>
<li>Snapshots of encrypted volumes are encrypted automatically</li>
<li>Volumes restored from encryted Snapshots are encrypted automatically</li>
<li>You can share snapshots, but only if they are unencrypted</li>
<li>These snapshots can be shared with other AWS accounts or made public</li>
<li><p>You can now encryted root device volumes upon creation of the EC2</p>
</li>
<li><p>Create a snapshot of the unencrypted root device volume.</p>
</li>
<li>Create a copy of the snapshot and select the encrypt option</li>
<li>Create an AMI from the encrypted snapshot</li>
<li>Use that AMI to launch new encrypted instances</li>
</ul>
<p><strong>Spot Instances and Spot Fleets [SAA-C02]</strong></p>
<ul>
<li>Spot instances can save up to 90% of the cost of On-Demand instances</li>
<li>Useful for any type of computing where you don’t need persistent storage</li>
<li>You can block Spot Instances from terminating by using Spot Block</li>
<li>A spot fleet is a collection of spot instances and, optionally, On-demand Instances</li>
</ul>
<p><strong>EC2 Hibernate [SAA-C02]</strong></p>
<ul>
<li>EC2 Hibernate preserves the in-memory RAM on persistent storage (EBS)</li>
<li>Much faster to boot up because you <em>do not need to reload the operating system</em></li>
<li>Instance RAM must be less than <em>150GB</em></li>
<li>Instance families include C3,C4,C5,M3,M4,M5,R3,R4,R5</li>
<li>Available for Windows, Amazon Linux 2 MAI, and Ubuntu</li>
<li>Instances can’t be hibernated for more than <em>60 Days</em></li>
<li>Available for <em>On-Demand instances</em> and <em>Reserved instances</em></li>
</ul>
<p><strong>CloudWatch 101</strong></p>
<ul>
<li>CloudWatch is used for monitoring Performance.</li>
<li>CloudWatch can monitor most os AWS as well as your applications that run on AWS.</li>
<li>CloudWatch with EC2 cill monitor events every 5 minutes by default.</li>
<li>You can have 1 minute intervals by turning on detailed monitoring.</li>
<li>You can create CloudWatch alarms which trigger notifications.</li>
<li>CloudWatch is all about performance. CloudTrail is all about auditing.</li>
</ul>
<p><strong>CloudWatch</strong></p>
<ul>
<li>Standard monitoring = 5 minutes</li>
<li>Detailed monitoring = 1 minute</li>
<li>Dashboards - Create dashboards to see what is happening with your AWS environment.</li>
<li>Alarms - Allows you to set Alarms that notify you when particular thresholds are hit.</li>
<li>Events - Cloudwatch Events helps you to respond to state changes in your AWS resources.</li>
<li>Logs - CloudWatch Logs helps you to aggregate, monitor, and store logs.</li>
</ul>
<p><strong>AWS (CLI)</strong></p>
<ul>
<li>You can interact with AWS from anywhere in the world just by using the CLI</li>
<li>You will need to set up access in IAM</li>
</ul>
<p><strong>Identity and Access Management Roles</strong><br> <em>Using IAM roles with EC2 makes it easier to securely access AWS service APIs from your EC2 instances. You can create an IAM role, assign it a set of permissions, and launch EC2 instances with the IAM role, and then AWS access keys with the specified permissions are automatically made available on those EC2 instances</em></p>
<ul>
<li>Roles are more secure than storing your access key and secret access key on individual EC2 instances.</li>
<li>Roles are easier to manage.</li>
<li>Roles can be assigned to an EC2 instance after it is created using both the console &amp; command line.</li>
<li>Roles are universal - You can use them in any region.</li>
</ul>
<p><strong>Using Bootstrap Scripts</strong></p>
<ul>
<li>Way of running things at the command line when your EC2 first boots.</li>
</ul>
<p><strong>Instance Metadata</strong></p>
<ul>
<li>Used to get information about an instance.<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">curl http://169.254.169.254/lastest/user-data you can see the bootstrap script.</span><br><span class="line">curl http://169.254.169.254/latest/meta-data/</span><br><span class="line">curl http://169.254.169.254/latest/meta-data/<span class="built_in">local</span>-ipv4</span><br><span class="line">curl http://169.254.169.254/latest/meta-data/public-ipv4</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>EFS [SAA-C02]</strong></p>
<ul>
<li>Supports the NFS v4 protocol</li>
<li>You only pay for the storage you use (no prep-provisioning required)</li>
<li>Can scale up to the petabytes</li>
<li>Can support thousands of concurrent NFS connections</li>
<li>Data is stored across multiple AZ’s within a region</li>
<li>Read After Write Consistency</li>
</ul>
<p><strong>Amazon FSx for Windows and Amazon FSx for Lustre [SAA-C02]</strong></p>
<ul>
<li><strong>EFS</strong> : When you need distributed, higly resilient storage for Linux instances and Linux-based applicatio<br>ns.</li>
<li><strong>Amazon FSx for Windows</strong>: When you nedd centralised storage for Windows-based applications such as Sharepoint, Microsoft SQL Server, Workspaces, IIS Web Server or any other native Microsoft Application.</li>
<li><strong>Amazon FSx for lustre</strong>: When you need high-speed, high capacity distributed storage. This will be for applications that do High Performance Compute (HPC), financial modelling etc. Remember that FSx for Lustre can store data directly to S3.</li>
</ul>
<p><strong>EC2 Placement Groups</strong></p>
<ul>
<li><strong>Clustered Placement Group</strong>: Low Network Latency / High Network Throughput</li>
<li><strong>Spread Placement Group</strong>: Individual Critical EC2 instances</li>
<li><strong>Partitioned</strong>: Multiple EC2 instances HDFS, HBase, and Cassandra</li>
<li>A Clustered placement group can’t span multiple Availability Zone</li>
<li>A spread placement and partitioned group can.</li>
<li>The name you specify for a placement group must be unique within your AWS account.</li>
<li>Only certain types of instances can be launched in a placement group (Compute Optimized, GPU, Memory Optimized, Storage Optimized)</li>
<li>AWS recommended homogenous instances within Clustered placement groups.</li>
<li>You can’t merge placement groups.</li>
<li>You can move an existing instance to a placement group. Before you move the instance, the instance must be in the stopped state. You can move or remove an instance using the AWS CLI or an AWS SDK, you can’t do it via the console yet.</li>
</ul>
<p><strong>HPC on AWS [SAA-C02]</strong></p>
<ul>
<li><strong>Data Transfer</strong>:</li>
<li>Snowball, Snowmobile (terrabytes/petabytes worth of data)</li>
<li>AWS DataSYnc to store on S3, EFS, FSx for Windows etc</li>
<li>Direct Connect</li>
<li><strong>Compute &amp; Networking</strong>:</li>
<li>EC2 instances that are GPU or CPU Optimized</li>
<li>EC2 Fleets (Spot Instance or Spot FLeets)</li>
<li>Placement groups (cluster placement groups)</li>
<li>Enhanced networking Single Root I/O virtualization (SR-IOV)</li>
<li>Elastic Network Adapters or intel 82599 Virtual Function (VF) interfaces</li>
<li>Elastic Fabric Adapters</li>
<li><strong>Storage</strong>:<ul>
<li>Instance-attached storage:<ul>
<li><strong>EBS</strong>: scale up to 64000 IOPS with Provisionned IOPS (PIOPS)</li>
<li><strong>Instance Store</strong>: Scale to millions of IOPS; low Latency</li>
</ul>
</li>
<li>Network Storage:<ul>
<li><strong>Amazon S3</strong>: Distributed object-based storage; not a file system.</li>
<li><strong>Amazon EFS</strong>: Scale IOPS based on total size, or use Provisionned IOPS</li>
<li><strong>Amazon FSx for Lustre</strong>: HPC-optimized distributed file system, millions of IOPS, which is also backed by S3</li>
</ul>
</li>
</ul>
</li>
<li><strong>Orchestration &amp; Automation</strong>: </li>
<li>AWS batch</li>
<li>AWS ParallelCluster <a href="https://aws.amazon.com/getting-started/hands-on/deploy-elastic-hpc-cluster/?nc1=h_ls" target="_blank" rel="noopener">Doc</a></li>
</ul>
<p><strong>AWS WAF [SAA-C02]</strong><br><em>In the exam you will be given different scenarios and you will be asked how to block malicious IP addresses</em></p>
<ul>
<li>Use AWS WAF</li>
<li>Use Network ACLs - (cf VPC Section)</li>
</ul>
<p><strong>EC2 Summary</strong><br><em>Amazon Elastic Compute Cloud is a web service that provides resizable compute capacity in the cloud. Amazon EC2 reduces the time required to obtain and boot new server instances to minutes, allowing you to quickly scale capacity, both up and down, as your computing requirements change</em></p>
<ul>
<li><strong>On Demand</strong>: Allows you to pay a fixed rate by the hour (or by the second) with no commitment.</li>
<li><strong>Reserved</strong>: Provides you with a capacity reservation, and offer a significant discount on the hourly charge for an instance. Contract Terms are 1 Year or 3 Year Terms.</li>
<li><strong>Spot</strong>: Enables you to bid whatever price you want for instance capacity, providing for even greater savings if your applications have flexible start and end times.</li>
<li><strong>Dedicated Hosts</strong>: Physical EC2 server dedicated for your use. Dedicated Hosts can help you reduce costs by allowing you to use your existing server-bound software licenses.</li>
</ul>
<p><em>If the Spot Instance is terminated by Amazon EC2, you will not be charged for a partial hour of usage. However, if you terminate the instance yourself, you will be charged for any hour in which the instance ran.</em></p>
<p><strong>EC2 Instances Types</strong>:<br>F - For FPGA<br>I - For IOPS<br>G - Graphics<br>H - High Disk Throughput<br>T - Cheap general purpose (Think T2 micro)<br>D - For Density<br>R - For RAM<br>M - Main choice for general purpose apps<br>C - For compute<br>P - Graphics (think Pics)<br>X - Extreme Memory<br>Z - Extreme Memory AND CPU<br>A - Arm-based workloads<br>U - Bare Metal</p>
<p><strong>EBS</strong>:</p>
<ul>
<li>EBS = hard disk drive on the cloud. Termination Protection is <strong>turned off</strong> by default, you must turn it on!</li>
<li>On a EBS-Backed instance, the <strong>default action is for the root EBS volume to be deleted</strong> when instance is terminated.</li>
<li>EBS Root Volumes of your DEFAULT AMI’s <strong>CAN</strong> be encrypted. You can also use a third party tool (such as bit locker etc) to encrypt the root volume, or this can be done when creating AMI’s (remember the lab) in the AWS console or using the API.<ul>
<li>Additional volumes can be encrypted.</li>
</ul>
</li>
</ul>
<p><strong>Security Group</strong></p>
<ul>
<li>All Inbound traffic is blocked by default.</li>
<li>All Outbound  traffic is allowed.</li>
<li>Changes to SG take effect immediately.</li>
<li>You can have any number of EC2 instances within a security group.</li>
<li>You can have multiple security groups attached to EC2 Instances.</li>
<li><strong>SG</strong> are <strong>STATEFULL</strong> (<strong>NACL</strong> are <strong>STATELESS</strong>)</li>
<li>If you create an inbound rule allowing traffic in, that traffic is automatically allowed back out again.</li>
<li>You cannot block specific IP addresses using SG, instead use NACL.</li>
</ul>
<p><strong>EBS Snapshots</strong>:</p>
<ul>
<li>Volumes exist on EBS. Think of EBS as a virtual hard disk.</li>
<li>Snapshots exist on S3. Think of snapshots as a photograph of the disk.</li>
<li>Snapshots are point in time copies of Volumes.</li>
<li><strong>Snapshots are incremental</strong> this means that only the blocks that have changed since your last snapshot are moved to S3.</li>
<li>If this is your first snapshot, it may take some time to create.</li>
<li>To create a snapshot for Amazon EBS volumes that serves as root devices, you should stop the instance before taking the snapshot.</li>
<li>However you can take a snap while the instance is running.</li>
<li>You can create AMI’s from both Volumes and snapshots</li>
<li>You can change EBS volume sizes on the fly, including changing the size and storage type.</li>
<li>Volumes will ALWAYS be in the same AZ as the EC2 instance.</li>
</ul>
<p><strong>Migrating EBS</strong></p>
<ul>
<li>To move an EC2 volume from one AZ to another, take a snapshot of it, create an AMI from the snapshot and the use the AMI to launch the EC2 instance in a new AZ.</li>
<li>To move an EC2 volume from one region to another, take a snapshot of it, create an AMI from the snapshot and then copy the AMI from one region to the other. Then use the copied AMI to launch the new EC2 instance in the new region.</li>
</ul>
<p><strong>EBS Encryption</strong></p>
<ul>
<li>Snapshots of encrypted volumes are encrypted automatically.</li>
<li>Volumes restored from encrypted Snapshots are encrypted automatically.</li>
<li>You can share snapshots, but only if they are unencrypted.</li>
<li>These snapshots can be shared with other AWS accounts or made public.</li>
</ul>
<p><strong>Exam tips</strong>:<br><strong>Root Device Volumes Can Now Be Encrypted. So if you have an unencrypted root device volume that needs to be encrypted do the following</strong></p>
<ul>
<li>Create a Snapshot of the unencrypted root device volume.</li>
<li>Create a copy of the Snapshot and select the encrypt option.</li>
<li>Create an AMI from the encrypted Snapshot.</li>
<li>Use that AMI to launch new encrypted instances.</li>
</ul>
<p><strong>EBS vs Instance Store</strong></p>
<ul>
<li>Instance Store Volumes are sometimes called Ephemeral Storage.</li>
<li>Instance store volumes cannot be stopped. If the underlying host fails, you will lose your data.</li>
<li>EBS backed instances can be stopped. You will not lose the data on this instance if it is stopped.</li>
<li>You can reboot both, you will not lose your data.</li>
<li>By default, both ROOT Volumes will be deleted on termination. However, with EBS volumes, you can tell AWS to keep the root device volume.</li>
</ul>
<p><strong>Encrypting Root Device Volumes</strong></p>
<ul>
<li>Create a snapshot oh the unencrypted root device volume.</li>
<li>Create a copy of the snapshot and select the encrypt option.</li>
<li>Create an AMI from the encrypted Snapshot</li>
<li>Use that AMI to launch new encrypted instances.</li>
</ul>
<p><strong>Enhanced Networking</strong><br><em>In the exam you will be given difreent scenarios and you will be asked to choose whether you should use ENI, EN or EFA</em></p>
<ul>
<li><strong>ENI</strong>: For basic Networking. Perhaps you need a separate management network to your production network or a separate logging network and you need to do this at law cost. In this scenario use multiple ENIs for each network.</li>
<li><strong>Enhanced Network</strong>: For when you need speeds between 10Gbps and 100Gbps. Anywhere you need reliable, high throughput.</li>
<li><strong>Elastic Fabric Adapter</strong>: For when you need to accelerate High Performance Computing (HPC) and machine learning applications or if you need to do an OS by-pass. If you see a scenario question mentioning HPC or ML and asking what network you want, choose EFA !</li>
</ul>
<p><strong>CloudWatch</strong></p>
<ul>
<li>Cloudwatch is used for monitoring performance.</li>
<li>Cloudwatch can monitor most of AWS as well as your applications that run on AWS.</li>
<li>Cloudwatch with EC2 will monitor events every 5 minutes by default.</li>
<li>You can have 1 minute intervals by turning on detailed monitoring.</li>
<li>You can create CloudWatch alarms which trigger notifications.</li>
<li>CloudWatch is all about performance. CloudTrail is all about auditing.</li>
<li>Dashboards - Creates awesome dashboards to see what is happening with your AWS environment.</li>
<li>Alarms - Allows you to set Alarms that notify you when particular thresholds are hit.</li>
<li>Events - CloudWatch events helps you to respond to state changes in your AWS resources.</li>
<li>Logs - CloudWatch Logs helps you to aggregate, monitor, and store logs.</li>
<li>CloudWatch monitors performance.</li>
<li>CloudTrail monitors API calls in the AWS platform.</li>
</ul>
<p><strong>The CLI</strong></p>
<ul>
<li>You can interact with AWS from anywhere in the world just by using the command line (CLI)</li>
<li>You will need to set up access in IAM</li>
<li>Commands themselves are not in the exam, but some basic commands will be useful to know for real life.</li>
</ul>
<p><strong>Roles</strong></p>
<ul>
<li>Roles are more secure than storing your access key and secret access key on individual EC2 instances.</li>
<li>Roles are easier to manage.</li>
<li>Roles can be assigned to an EC2 instance after it is created using both the console &amp; command line.</li>
<li>Roles are universal - you can use them in any region.</li>
</ul>
<p><strong>Bootstrap SCripts</strong></p>
<ul>
<li>Bootstrap scripts run when an EC2 instance first boots.</li>
<li>Can be a powerful way of automating software installs and updates.</li>
</ul>
<p><strong>Instance Meta Data &amp; User Data</strong></p>
<ul>
<li>Used to get information about an instance (such as public IP)<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl http://169.254.169.254/latest/meta-data/</span><br><span class="line">curl http://169.254.169.254/latest/user-data/</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>EFS</strong></p>
<ul>
<li>Supports the NFSv4 pprotocol.</li>
<li>You only pay for the storage you use (no pre-provisioning required.)</li>
<li>Can scale up to the petabytes.</li>
<li>Can support thousands of concurrent NFS connections.</li>
<li>Data is stored accross multiple AZ’s within a region.</li>
<li>Read After Write Consistency.</li>
</ul>
<p><em>In the exam you’ll be given different scenarios and asked to choose whether you should use an EFS, FSx for Windows or FSx for Lustre.</em></p>
<ul>
<li><strong>EFS</strong>: When you need distributed, highly resilient storage for Linux instances and Linux-based applications.</li>
<li><strong>Amazon FSx for Windows</strong>: When you need centralised storage for Windows-based applications such as Sharepoint, Microsoft SQL Server, Workspaces, IIS Web Server or any other native Microsoft Application.</li>
<li><strong>Amazon FSx for Lustre</strong>: When you need high-speed, high-capacity distributed storage. This will be for applications that do High Performance Compute (HPC), financial modelling etc. Remember that FSx for Lustre can store data directly on S3.</li>
</ul>
<p><strong>EC2 Placement Groups</strong><br><em>Three types of Placement Group</em></p>
<ul>
<li><strong>Clustered Placement Group</strong><ul>
<li>Low Network Latency / High Network Throughputi.</li>
</ul>
</li>
<li><strong>Spread Placement Group</strong><ul>
<li>Individual Critical EC2 instances.</li>
</ul>
</li>
<li><strong>Partitioned</strong><ul>
<li>Multiple EC2 instances HDFS, HBASE, and Cassandra.</li>
</ul>
</li>
<li>A Clustered placement group can’t span multiple AZs.</li>
<li>A Spread placement and Partitioned can.</li>
<li>The name you specify for a placement group must be unique within you AWS account.</li>
<li>Only certain types of instances can be launched in a placement group (Compute Optimized, GPU, Memory Optimized, Storage Optimized)</li>
<li>AWS recommend homogenous instances within Clustered Placement groups.</li>
<li>You can’t merge placement groups.</li>
<li>You can’t move an existing instance into a placement group. You can create an AMI from your existing instance, and the launch a new instance from the AMI into a placement group.</li>
</ul>
<h1 id="Databases-101"><a href="#Databases-101" class="headerlink" title="Databases 101"></a>Databases 101</h1><p><strong>RDS</strong> (OLPT) consist of online transaction processing:</p>
<ul>
<li>SQL</li>
<li>MySQL</li>
<li>PostgreSQL</li>
<li>Oracle</li>
<li>Aurora</li>
<li><p>MariaDB</p>
</li>
<li><p>Dynamodb (NoSQL)</p>
</li>
<li><p>RedShift (OLAP) Online Analytic Processing: using for Business Intelligence or Date Warehousing.</p>
</li>
<li><p>Elasticache : to speed up performance of existing databases (frequent identical queries)</p>
<ul>
<li>Memcached</li>
<li>Redis</li>
</ul>
</li>
<li><p>RDS runs on VM</p>
</li>
<li>You cannot log in to theses operating systems however.</li>
<li>Patching of the RDS Operating System and DB is Amazon’s responsibility</li>
<li>RDS is NOT Serverless.</li>
<li>Aurora Serverless is Serverless.</li>
</ul>
<p><strong>RDS: Backups, Multi-AZ, and Read Replicas</strong></p>
<ul>
<li>TODO….</li>
</ul>
<p><strong>RDS: Backups, Multi-AZ, and Read Replicas - Lab</strong></p>
<ul>
<li>There are two different types of Backups for RDS:<ul>
<li>Automated Backups</li>
<li>Databases Snapshots</li>
</ul>
</li>
<li>Read Replicas:<ul>
<li>Can be Multi-AZ</li>
<li>Used to increase performance.</li>
<li>Must have backups turned on.</li>
<li>Can be in different regions.</li>
<li>Can be MySQL, PostrgeSQL, MariaDB, Oracle, Aurora.</li>
<li>Can be promoted to master, this will break the Read Replica.</li>
</ul>
</li>
<li>MutliAZ:<ul>
<li>Used for DR(Disaster Recovery)</li>
<li>You can force a failover from one AZ to another by rebooting the RDS instance.</li>
</ul>
</li>
</ul>
<p><strong>DynamoDB</strong></p>
<ul>
<li>Stored on SSD storage.</li>
<li>Spread across 3 geographically distinct data centers.</li>
<li>Eventual Consistent Reads (Default).</li>
<li>Strongly Consistent Reads.</li>
<li><strong>Eventual Consistent Reads</strong>: Consistency across all copies of data is usually reached within a second. Repeating a read after a short time should return updated data. (Best Read Performance)</li>
<li><strong>Strongly Consistent Reads</strong>: A strongly consistent read returns a result that reflects all writes that received a successful response prior to the read.</li>
</ul>
<p><strong>Advanced DynamoDB [SAA-C02]</strong></p>
<ul>
<li><strong>DAX</strong> (DynamoDB Accelerator)</li>
<li>Fully managed, highly available, in-memory cache</li>
<li>10x performance improvement</li>
<li>Reduces request time from milliseconds to <strong>microseconds</strong> - even under load !</li>
<li>No need for developers to manage caching logic.</li>
<li>Compatible with DynamoDB API calls</li>
<li><strong>Transactions</strong>:<ul>
<li>Multiple “all-or-nothing” operations.</li>
<li>Financial transactions.</li>
<li>Fulfilling orders.</li>
<li>Two underlying reads or writes - prepare/commit</li>
<li>Up to 25 items or 4 MB of data.</li>
</ul>
</li>
<li><strong>On-Demand capacity</strong>:<ul>
<li><strong>Pay-per-request</strong> pricing.</li>
<li>Balance cost and performance.</li>
<li>No minimum capacity.</li>
<li>No charge for read/write - only storage and backups.</li>
<li><strong>Pay more per request</strong> than with provisioned capacity.</li>
<li>Use for new product launches.</li>
</ul>
</li>
<li><strong>Backup and Restore</strong>:<ul>
<li><strong>On-Demand backup and restore</strong>:<ul>
<li>Full backups at any time.</li>
<li>Zero impact on table performance or availability</li>
<li>Consistent within seconds and <strong>retained until deleted</strong></li>
<li>Operates within same region as the source table.</li>
</ul>
</li>
<li><strong>Point in Time Recovery (PITR)</strong>:<ul>
<li>Protects against accidental writes or deletes.</li>
<li>Restore to any point in the last <strong>35 days</strong>.</li>
<li>Incremental backups.</li>
<li>Not enabled by default.</li>
<li>Latest restorable: <strong>five minutes</strong> in the past.</li>
</ul>
</li>
</ul>
</li>
<li><strong>Streams</strong><ul>
<li>Time-ordered sequence of item-level changes in a table.</li>
<li>Stored for 24 hours.</li>
<li>Inserts, updates and deletes.</li>
<li>Combine with Lambda functions for functionality like stored procedures.</li>
</ul>
</li>
<li><p><strong>Global Tables</strong>:</p>
<ul>
<li><strong>Managed Multi-Master, Multi-Region Replication</strong>:<ul>
<li>Globally distributed applications.</li>
<li>Based on DynamoDB streams.</li>
<li>Multi-region redundancy for DR or HA.</li>
<li>No application rewrites.</li>
<li>Replication latency under <strong>one second</strong></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Database Migration Servicve (DMS)</strong>:</p>
</li>
<li><p><strong>Security</strong>:</p>
<ul>
<li>Encryption at REST using KMS.</li>
<li>Site-to-site VPN.</li>
<li>Direct Connect (DX).</li>
<li>IMA policies and roles.</li>
<li>Fine-grained access.</li>
<li>CloudWatch and CloudTrail.</li>
<li>VPC Endpoint.</li>
</ul>
</li>
</ul>
<p><strong>Redshift</strong>:<br><em>Redshift is a petabyte-scale data warehouse product available within AWS. It’s capable of being used for ad-hoc warehousing/analytics or long-running deployments.</em></p>
<ul>
<li>RedShift is used for Business Intelligence.</li>
<li>OLAP: Online Analytic Processing: using for Business Intelligence or Date Warehousing.</li>
<li>Single Node (160G)</li>
<li>Multi-Node<ul>
<li>Leader Node (manages client connections and receives queries)</li>
<li>Compute Node (store data and perform queries and computations). Up to 128 Compute Nodes.</li>
</ul>
</li>
<li>MPP: Massive Parallel Processing.</li>
<li>Backups:<ul>
<li>Enable by default with 1 day retention period.</li>
<li>Maximum retention period is 35 days.</li>
<li>Redshift always attempts to maintain at least three copies of your data (the original and replica on the compute nodes and a backup in Amazon S3).</li>
<li>RedShift can also asynchronously replicate your snapshots to S3 in another region for disaster recovery.</li>
</ul>
</li>
<li>Encrypted in transit using SSL</li>
<li>Encrypted at REST using AES-256 encryption.</li>
<li>By default Redshift takes care of key management.<ul>
<li>Manage your own keys through HSM.</li>
<li>AWS key Management Service.</li>
</ul>
</li>
<li>Currently only available in 1 AZ</li>
<li>Can restore snapshots to new AZs in the event of an outage.</li>
</ul>
<p><strong>Aurora [SAA-C02]</strong><br><em>Aurora is a custom-designed relational database engine that forms part of RDS. Rather than an evolution, Aurora significantly replaces much of the traditional MySQL and PostgreSQL architecture in favor of cluster and shared storage architecture, which is more scalable and resilient with much higher performance.</em></p>
<ul>
<li>2 copies of your data are contained in each AZ, with minimum of 3 AZ. 6 copies of your data.</li>
<li>You can share Aurora Snapshots with other AWS account.</li>
<li>3 types of replicas available:<ul>
<li>Aurora Replicas (15) (Automated failover is only available with Aurora Replicas.)</li>
<li>MySQL replicas (5)</li>
<li>PostgresQL replicas. (1)</li>
</ul>
</li>
<li>Aurora has automated backups turned on by default. You can also take snapshots with Aurora. You can share these snapshots with other AWS accounts.</li>
<li>Use Aurora Serverless if you want a simple, cost effective option for infrequent, intermittent, or unpredictable workloads.</li>
</ul>
<p><strong>Elasticache</strong></p>
<ul>
<li>Use Elasticache to increase database and web application performance.</li>
<li>Redis is Multi-AZ.</li>
<li>You can do back ups and restores of Redis.</li>
</ul>
<p><strong>Database Migration Service (DMS) [SAA-C02]</strong><br><em>In this Part, we’ll cover Database Migration Service (DMS), which can help with homogeneous migrations as well as migrations between different database engines, such as Oracle or SQL Server, to Amazon Aurora or DynamoDB.</em></p>
<ul>
<li>DMS allows you to <strong>migrate databases</strong> from one source to AWS.</li>
<li>The source can either be on-premises, or inside AWS itself or another cloud provider such as Azure.</li>
<li>You can do <strong>homogenous</strong> migrations (same DB engines) or <strong>heterogeneous</strong> migrations.</li>
<li>If you do heterogeneous migration, you will need the <strong>AWS Schema Conversion Tool (SCT)</strong></li>
</ul>
<p><strong>Caching Strategies on AWS [SAA-C02]</strong><br><em>In this Part, we’ll cover the various caching strategies commonly found in AWS, along with the benefits and tradeoffs of each.</em><br><em>Caching is a balancing act between up-to-date, accurate information and latency. We can use the following services to <strong>cache on AWS</strong></em>:</p>
<ul>
<li>CloudFront</li>
<li>API Gateway</li>
<li>ElastiCache - Memcached and Redis.</li>
<li>DynamoDB Accelerator (DAX)</li>
</ul>
<p><strong>EMR Overview [SAA-C02]</strong><br><em>Elastic MapReduce is an AWS-managed implementation of the Apache Hadoop ecosystem of products. This lesson walks through the high-level architecture of the product.</em></p>
<ul>
<li>EMR is used for <strong>Big Data processing</strong></li>
<li>Consists of a <strong>master node</strong>, a <strong>core node</strong>, and (optionally) a <strong>task node</strong>.</li>
<li>By default, log data is <strong>stored on the master node</strong>.</li>
<li>You can configure replication to S3 on <strong>five-minutes intervals for all log data from the master node</strong>; however, this can only be configured when creating the cluster for the first time.</li>
</ul>
<p><strong>Databases Summary</strong></p>
<ul>
<li><strong>RDS</strong> (OLTP)</li>
<li>DynamoDB (NoSQL)</li>
<li>RedShift OLAP</li>
<li>ElastiCache (memcached / Redis)</li>
<li>RDS runs on VM</li>
<li>You cannot log in to these operating systems however.</li>
<li>Patching of the RDS Operating System and DB is Amazon’s responsibility.</li>
<li>RDS is NOT Serverless</li>
<li>Aurora Serverless Is Serverless</li>
<li>2 types of Backups for RDS:<ul>
<li>Automated Backups</li>
<li>Database Snapshots</li>
</ul>
</li>
<li>Read Replicas can be Multi-AZ<ul>
<li>Used to increas performance.</li>
<li>Must have backups turned on.</li>
<li>Can be in different regions.</li>
<li>Can be MySQL, PostrgeSQL, MariaDB, Oracle, Aurora.</li>
<li>Can be promoted to master, this will break the Read Replica.</li>
<li>MSSQL is now available to be used with Read Replicas.</li>
</ul>
</li>
<li>MutliAZ:<ul>
<li>Used for DR.</li>
<li>You can force a failover from one AZ to another by rebooting the RDS instance.</li>
</ul>
</li>
<li><strong>DynamoDB</strong>:<ul>
<li>Stored on SSD storage.</li>
<li>Spread accross 3 geographically distinct data centres.</li>
<li>Eventual Consistent Reads (default)</li>
<li>Strongly Consistent Reads.</li>
</ul>
</li>
<li><strong>RedShift</strong>:<ul>
<li>Is used for Business intelligence.</li>
<li>Available in only 1 AZ.</li>
<li>Backups are enabled by default with a 1 day retention period.</li>
<li>Maximum retention period is 35 days.</li>
<li>Redshift always attempts to maintain at least 3 copies of your data (the original and replica on the compute nodes and backup in Amazon S3)</li>
<li>RedShift can also asynchronously replicate you snapshots to S3 in another region for disaster recovery.</li>
</ul>
</li>
<li><strong>Aurora</strong>:<ul>
<li>2 copies of your data are contained in each AZ, with minimum of 3 AZ. 6 copies of your data.</li>
<li>You can share Aurora snapshots with other AWS accounts.</li>
<li>3 types of replicas available. Aurora Replicas, MySQL replicas &amp; PostgreSQL replicas. Automated failover is only available with Aurora Replicas.</li>
<li>Aurora has automated backups turned on by default. You can also take snapshots with Aurora. You can share these snapshots with other AWS accounts.</li>
<li>Use Aurora Serverless if you want a simple, cost effective option for infrequent, intermittent, or unpredictable workloads.</li>
</ul>
</li>
<li><strong>Elasticache</strong>:<ul>
<li>Use ElastiCache to increase database and web application performance.</li>
<li>Redis is Multi-AZ</li>
<li>You can do back ups and restores of Redis.</li>
<li>If you need to scale horizontally, use Memcached.</li>
</ul>
</li>
</ul>
<h1 id="Advanced-IAM"><a href="#Advanced-IAM" class="headerlink" title="Advanced IAM"></a>Advanced IAM</h1><p><strong>AWS Directory Service [SAA-C02]</strong><br><em>Here we’ll go over AWS Directory Service, which offers multiple identity management solutions — such as Microsoft Managed AD, AD Connector, and Simple AD — to suit different use cases. Each solves a different part of the problem for IT admins and DevOps engineers in managing user access to AWS resources.</em></p>
<ul>
<li>Family of managed services.</li>
<li>Connect AWS resources with on-premises AD.</li>
<li>Standalone directory in the cloud.</li>
<li>Use existing corporate credentials.</li>
<li>SSO to any domain-joined EC2 instance.</li>
<li><strong>AD compatible</strong>:<ul>
<li>Managed Microsoft AD (aka, Directory Service for Microsoft Active Directory).</li>
<li>AD Connector.</li>
<li>Simple AD.</li>
</ul>
</li>
<li><strong>NOT AD Compatible</strong>:<ul>
<li>Cloud Directory.</li>
<li>Cognito user pool.</li>
</ul>
</li>
</ul>
<p><strong>IAM Policies [SAA-C02]</strong><br><em>we’ll discuss the structure of IAM policies and the logic by which their rules are applied when determining access to AWS resources</em></p>
<ul>
<li>Amazon Resource Name (ARN) all begin with: arn:partition:service:region:account_id:</li>
<li>JSON Document that defines permission.</li>
<li>Identity Policy.</li>
<li>Resource Policy.</li>
<li>No effect until attached.</li>
<li>A Policy is a list of <strong>statements</strong> (EAR PEAR).</li>
<li>Each statements matches an <strong>AWS API request</strong></li>
<li>Effect is either Allow or Deny.</li>
<li>Not explicitly allowed == <strong>implicitly denied</strong>.</li>
<li>Explicitly deny &gt; everything else</li>
<li>Only attached policies have effect.</li>
<li>AWS <strong>joins</strong> all applicable policies.</li>
<li>AWS-managed VS. Customer-managed</li>
<li><strong>Permission Boundaries</strong>:<ul>
<li>Used to <strong>delegate</strong> administration to other users.</li>
<li>Prevent <strong>privilege escalation</strong> or <strong>unnecessarily broad permissions</strong>.</li>
<li>Control <strong>maxium</strong> permissions an IAM policy can grant.</li>
</ul>
</li>
<li><strong>Use cases</strong>:<ul>
<li>Developers creating roles for Lambda functions.</li>
<li>Application owners creating roles for EC2 instances.</li>
<li>Admins creating ad hoc users</li>
</ul>
</li>
</ul>
<p><strong>AWS Resource Access Manager (RAM) [SAA-C02]</strong>:</p>
<ul>
<li>RAM allows you to securely <strong>share AWS resources</strong> with any AWS account or within your AWS Organization.</li>
</ul>
<p><strong>AWS Single Sign-On [SAA-C02]</strong>:<br><em>AWS SSO provides a user portal so users can find and access the roles they can assume in their assigned AWS accounts and business applications in one place. AWS SSO offers pre-configured SAML integrations to many business applications, including Salesforce, G Suite, and Office 365.</em></p>
<ul>
<li>If you SAML 2.0 in a exam question look for SSO in one of the answer.</li>
</ul>
<p><strong>Advanced IAM Summary [SAA-C02]</strong></p>
<ul>
<li><strong>Directory Service</strong>:</li>
<li>Active directory.</li>
<li>Connect AWS resources with on-premises AD.</li>
<li>SSO to any domain-joined EC2 instance</li>
<li>AWS Managed Microsoft AD</li>
<li>AD trust (use to extend AD inside AWS to your on-premises environment)</li>
<li>AWS VS. Customer responsibility.<ul>
<li><strong>Simple AD</strong>:<ul>
<li>Does not support trusts (can’t join simple AD to you on-premises AD) If you want to do that you need to use :</li>
</ul>
</li>
</ul>
</li>
<li><strong>AD Connector</strong>: (this is a directory gateway or proxy for your on-premises AD)</li>
<li><strong>Cloud directory</strong>:(service for developers looking to work with hierarchical data), it has nothing to do with Microsoft AD.</li>
<li><p><strong>Cognito user pools</strong>: (manage user directory that works with social media identities)</p>
</li>
<li><p><strong>AD compatible</strong>:</p>
</li>
<li>Managed Microsoft AD (aka, Directory Service for Microsoft Active Directory).</li>
<li>AD Connector.</li>
<li>Simple AD.</li>
<li><strong>NOT AD Compatible</strong>:</li>
<li>Cloud Directory.</li>
<li><p>Cognito user pool.</p>
</li>
<li><p><strong>IAM Policies</strong>:</p>
</li>
<li>arn</li>
<li>IAM Policy structure</li>
<li>Effect/Action/Resource</li>
<li>Identity VS. resource policies</li>
<li>Policy evaluation logic</li>
<li>AWS managed VS. Customer managed</li>
<li><p>Permission boundaries.</p>
</li>
<li><p><strong>Ressource Access Manager (RAM)</strong></p>
</li>
<li>Resource sharing between accounts.</li>
<li>Individual accounts and AWS Organizations.</li>
<li><p>Types of resources you can share.</p>
</li>
<li><p><strong>Single-Sign-On (SSO)</strong></p>
</li>
<li>Centrally manage access</li>
<li>Exemple: G Suite, Office 365, Salesforce</li>
<li>Use existing identities.</li>
<li>Account-level permissions</li>
<li>SAML </li>
</ul>
<h1 id="Route-53"><a href="#Route-53" class="headerlink" title="Route 53"></a>Route 53</h1><p><em>Work in progress \o/</em></p>
<h1 id="VPC"><a href="#VPC" class="headerlink" title="VPC"></a>VPC</h1><p><em>Work in progress \o/</em></p>
<h1 id="HA-Architecture"><a href="#HA-Architecture" class="headerlink" title="HA Architecture"></a>HA Architecture</h1><p><em>Work in progress \o/</em></p>
<h1 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h1><p><strong>SQS [SAA-C02]</strong><br><em>Simple Queue Service (SQS) provides standard or FIFO queues as a service. It helps applications scale by allowing decoupling of application components and inter-process, -service, and -server messaging.</em></p>
<ul>
<li>SQS is pull-based, not pushed-based (you have to have an EC2 pulling the message out of the queue)</li>
<li>Messages are 256KB in size.</li>
<li>Messages can be kept in the queue from 1 minute to 14 days; the default retention period is 4 days.</li>
<li>Visibility Timeout is the amount of time that the message in invisible in the SQS queue after a reader picks up that message. Provided the job is processed before the visibility timeout expires, the message will then be deleted from the queue. If the job is not processed within that time, the message will become visible again and another reader will process it. This could result in the same message being delivered twice.</li>
<li>Visibility timeout maximum is 12 Hours.</li>
<li>SQS guarantees that your messages will be processed at least once.</li>
<li>SQS <strong>long polling</strong> is a way to retrieve messages from your Amazon SQS queue. While the regular short polling returns immediately (even if the message queue being polled is empty), long polling doesn’t return a response until a message arrives in the message queue, or the long poll times out.</li>
<li>Any time you see a scenario based question about <strong>“decoupling”</strong> your infrastructure - <strong>think SQS.</strong></li>
</ul>
<p><em>Work in progress \o/</em></p>
<h1 id="Security"><a href="#Security" class="headerlink" title="Security"></a>Security</h1><p><em>Work in progress \o/</em></p>
<h1 id="Serverless"><a href="#Serverless" class="headerlink" title="Serverless"></a>Serverless</h1><p><em>Work in progress \o/</em></p>
<p>To be continued…&gt;(((°&gt;…</p>
<p>Sources:<br><a href="https://linuxacademy.com/cp/modules/view/id/630" target="_blank" rel="noopener">LinuxAcademy: AWS Certified Solutions Architect Associate SAA-C02</a><br><a href="https://aws.amazon.com/faqs/" target="_blank" rel="noopener">AWS Faq</a><br><a href="https://learn.cantrill.io/p/aws-certified-solutions-architect-associate-saa-c02" target="_blank" rel="noopener">learn.cantrill.io</a><br><a href="https://docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html" target="_blank" rel="noopener">AWS Well-Architected Framework</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://91.208.181.214/2020/09/08/AWS-SAAC-02/" data-id="ckf6o8c42002xxsiyishqrap3" class="article-share-link">Partager</a>
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AWS/">AWS</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-blocklist-IP-address-with-Varnish" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/09/01/blocklist-IP-address-with-Varnish/" class="article-date">
  <time datetime="2020-09-01T20:21:42.000Z" itemprop="datePublished">2020-09-01</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Varnish/">Varnish</a>►<a class="article-category-link" href="/categories/Varnish/blocklist-IP-address-with-Varnish/">blocklist IP address with Varnish</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/09/01/blocklist-IP-address-with-Varnish/">blocklist IP address with Varnish</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="Prerequisites-and-example-of-blocking-IP-address-via-Varnish-behind-an-NginX-or-an-ALB-CDN-etc-…"><a href="#Prerequisites-and-example-of-blocking-IP-address-via-Varnish-behind-an-NginX-or-an-ALB-CDN-etc-…" class="headerlink" title="Prerequisites and example of blocking IP address via Varnish behind an NginX or an ALB, CDN, etc …"></a>Prerequisites and example of blocking IP address via Varnish behind an NginX or an ALB, CDN, etc …</h3><p><strong>Prerequisites </strong></p>
<p>Make sure that the <em>X-Forwarded-For</em> Header is returned by NginX, ALB, CDN, etc.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Nginx vhost conf</span></span><br><span class="line">server &#123;</span><br><span class="line">    [...]</span><br><span class="line">    location / &#123;</span><br><span class="line">        [...]</span><br><span class="line">        proxy_set_header    X-Forwarded-For <span class="variable">$proxy_add_x_forwarded_for</span>;</span><br><span class="line">        [...]</span><br><span class="line">        proxy_pass  http://127.0.0.1:80/;</span><br><span class="line">    &#125;</span><br><span class="line">    [...]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>Setting up filtering</strong></p>
<p>On the Varnish side, the filtering will go through the following 3 steps:</p>
<ol>
<li>Declaration or use of an ACL for the blocklist</li>
<li>Cleaning the X-Forwarded-For header</li>
<li>IP Cast, Test and Block</li>
</ol>
<p><strong>ACL Blocklist</strong></p>
<p>After vcl_init and other acl directives, and before vcl_recv, make sure it is present or create the blocklist ACL and fill in the IPs to blocklist:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[...]</span><br><span class="line">acl whitelist &#123;</span><br><span class="line">    [...]</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">acl blacklist &#123;</span><br><span class="line">    <span class="string">"X.X.X.X"</span>.</span><br><span class="line">    <span class="string">"Y.Y.Y.Y"</span>/24.</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">sub vcl\_recv &#123;</span><br><span class="line">    [...]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>Cleaning the XFF Header</strong></p>
<p>The created ACL contains objects of type IP, however, the XFF Header is a character string.<br>It is therefore necessary to cast the string into an IP object using the ip function of the sdt module.</p>
<p>However, this function takes as input only one IP (as a string) <a href="see https://varnish-cache.org/docs/6.1/reference/vmod_std.generated.html#func-ip">Source: Varnish doc</a> , so you have to clean up the X-Forwarded-For Headers so that they contain only one IP:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">sub vcl_recv &#123;</span><br><span class="line">    [...]</span><br><span class="line">    <span class="comment"># forward client IP</span></span><br><span class="line">    <span class="keyword">if</span> (req.restarts == 0) &#123;</span><br><span class="line">        <span class="comment"># </span></span><br><span class="line">	<span class="comment"># We only get the first IP of the XFF header if it exists,</span></span><br><span class="line">	<span class="comment"># otherwise we take the IP present in client.ip</span></span><br><span class="line">	<span class="comment">#</span></span><br><span class="line">        <span class="keyword">if</span> (req.http.X-Forwarded-For) &#123; <span class="built_in">set</span> req.http.X-Forwarded-For = regsub(req.http.X-Forwarded-For,<span class="string">"^(([0-9]&#123;1,3&#125;\.)&#123;3&#125;([0-9]&#123;1,3&#125;)).*"</span>,<span class="string">"\1"</span>); &#125;</span><br><span class="line">        <span class="keyword">else</span> &#123; <span class="built_in">set</span> req.http.X-Forwarded-For = client.ip; &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    [...]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>It is possible not to overwrite the XFF Header and instead create another Header which will be used for all the ACL tests, it will then be necessary to replace the set <em>req.http.X-Forwarded-For</em> by set <em>req.http.My-Custom-Header-Name</em> and be careful to replace it also wherever an ACL match is made.</p>
<p>/!\ Ideally, this rule should be placed just after the loadbalancer’s health_check (httpprobe) so that all subsequent rules can benefit from it /!\</p>
<p><strong>IP Cast, Test, and Block</strong></p>
<p>Finally, you just have to cast the IP (string) into an IP object, check if it matches the ACL Blocklist and, if necessary, block it ;) </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sub vcl_recv &#123;</span><br><span class="line">    [...]</span><br><span class="line">    <span class="comment"># BlockList</span></span><br><span class="line">    <span class="keyword">if</span> (std.ip(req.http.X-Forwarded-For,<span class="string">"0.0.0.0"</span>) ~ blacklist) &#123;</span><br><span class="line">        <span class="built_in">return</span>(synth(403, <span class="string">"Not allowed."</span>));</span><br><span class="line">    &#125;</span><br><span class="line">    [...]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Warning ! You must place this directive after cleaning the XFF header, and not before, otherwise it will not work !</p>
<p><strong>Debug &amp; Test</strong></p>
<p>If the filtering does not work, it is possible to test the content of the XFF Header with varnishncsa as follow: </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">varnishncsa -F <span class="string">'%&#123;X-Forwarded-For&#125;i %l %u %t "%r" %s %b "%&#123;Referer&#125;i" "%&#123;User-agent&#125;i"'</span></span><br></pre></td></tr></table></figure>
<p>or to check which directives and directive results were applied to the request with varnishlog.</p>
<p>Happy blocking \o/</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://91.208.181.214/2020/09/01/blocklist-IP-address-with-Varnish/" data-id="ckf6o8c0w0009xsiy549mpmy4" class="article-share-link">Partager</a>
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/varnish/">varnish</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-Nginx-ModSecurity" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/07/22/Nginx-ModSecurity/" class="article-date">
  <time datetime="2020-07-22T21:21:42.000Z" itemprop="datePublished">2020-07-22</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Nginx/">Nginx</a>►<a class="article-category-link" href="/categories/Nginx/Playing-with-Nginx-and-Dynamic-Modules/">Playing with Nginx and Dynamic Modules</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/07/22/Nginx-ModSecurity/">Nginx-ModSecurity</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Yo this is a work in progress…highly inspired by <a href="https://linuxacademy.com/" target="_blank" rel="noopener">Linuxacademy</a>.<br>So let’s play with ModeSecurity and ModeSecurity-nginx.</p>
<p>OS: Ubuntu 20.04<br>CPU: 2<br>RAM: 2G<br>NGINX-VERSION=1.18.0</p>
<h2 id="install-pre-requisites"><a href="#install-pre-requisites" class="headerlink" title="install  pre-requisites:"></a>install  pre-requisites:</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt-get install git build-essential libpcre3 libpcre3-dev libssl-dev libtool autoconf apache2-dev libxml2-dev libcurl4-openssl-dev automake pkgconf zlib1g zlib1g-dev</span><br></pre></td></tr></table></figure>
<h2 id="clone-compile-Install-core-library-for-ModeSecurity"><a href="#clone-compile-Install-core-library-for-ModeSecurity" class="headerlink" title="clone / compile / Install core library for ModeSecurity"></a>clone / compile / Install core library for ModeSecurity</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /opt</span><br><span class="line">git <span class="built_in">clone</span> --depth 1 -b v3/master https://github.com/SpiderLabs/ModSecurity</span><br><span class="line"><span class="built_in">cd</span> ModSecurity</span><br><span class="line">git submodule init</span><br><span class="line">git submodule update</span><br><span class="line">./build.sh</span><br><span class="line">./configure</span><br><span class="line">make</span><br><span class="line">make install</span><br></pre></td></tr></table></figure>
<p><em>/!\ FYI compilation is quite long with this spec: Ubuntu 20.04 (2CPUs and 2G RAM) /!\</em></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">time make install</span><br><span class="line">real    14m16.427s &lt;==&gt; :( </span><br><span class="line">user    12m54.898s</span><br><span class="line">sys     0m42.830s</span><br></pre></td></tr></table></figure>
<h2 id="Donwload-Nginx-source-code-Build-a-dynamic-module-using-ModeSecurity-nginx"><a href="#Donwload-Nginx-source-code-Build-a-dynamic-module-using-ModeSecurity-nginx" class="headerlink" title="Donwload Nginx source code / Build a dynamic module using ModeSecurity-nginx"></a>Donwload Nginx source code / Build a dynamic module using ModeSecurity-nginx</h2><p><em>Downloading ModeSecurity-nginx</em><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /opt</span><br><span class="line">git <span class="built_in">clone</span> --depth 1 https://github.com/SpiderLabs/ModSecurity-nginx.git</span><br></pre></td></tr></table></figure></p>
<p><em>verify Nginx version and download Sources</em><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#check-version</span></span><br><span class="line">nginx -v</span><br><span class="line">nginx version: nginx/1.18.0</span><br><span class="line">NGINX_VERSION=1.18.0</span><br><span class="line">wget http://nginx.org/download/nginx-<span class="variable">$NGINX_VERSION</span>.tar.gz</span><br><span class="line">tar zxvf nginx-<span class="variable">$NGINX_VERSION</span>.tar.gz</span><br><span class="line"></span><br><span class="line"><span class="comment">#Configure and build dynamic module</span></span><br><span class="line"><span class="built_in">cd</span> nginx-1.18.0</span><br><span class="line">./configure --with-compat --add-dynamic-module=../ModSecurity-nginx</span><br><span class="line">make modules</span><br><span class="line">cp objs/ngx_http_modsecurity_module.so /etc/nginx/modules/</span><br></pre></td></tr></table></figure></p>
<p><em>Load ModSecurity dynamic module</em><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load ModSecurity dynamic module outside events or htpp context</span></span><br><span class="line">load_module /etc/nginx/modules/ngx_http_modsecurity_module.so;</span><br></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mkdir /etc/nginx/modsecurity</span><br><span class="line">cp /opt/ModSecurity/modsecurity.conf-recommended /etc/nginx/modsecurity/modsecurity.conf</span><br><span class="line">cp /opt/ModSecurity/unicode.mapping /etc/nginx/modsecurity/unicode.mapping</span><br><span class="line"></span><br><span class="line">vim /etc/nginx/modsecurity/modsecurity.conf +230</span><br><span class="line">and change the SecAuditLog path <span class="keyword">for</span> something like /var/<span class="built_in">log</span>/nginx/modsec_audit.log</span><br><span class="line">and add those two lines inside your vhost:</span><br><span class="line"></span><br><span class="line">modsecurity on;</span><br><span class="line">modsecurity_rules_file /etc/nginx/modsecurity/modsecurity.conf;</span><br></pre></td></tr></table></figure>
<h2 id="Reload-Nginx-and-enjoy-o"><a href="#Reload-Nginx-and-enjoy-o" class="headerlink" title="Reload Nginx and enjoy \o/"></a>Reload Nginx and enjoy \o/</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nginx -t</span><br><span class="line">systemctl reload nginx</span><br></pre></td></tr></table></figure>
<p>Work is still in progress but Nginx is reloading and everything looks great for now. :) </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://91.208.181.214/2020/07/22/Nginx-ModSecurity/" data-id="ckf6o8c0q0005xsiyluuw0dy1" class="article-share-link">Partager</a>
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ModSecurity/">ModSecurity</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/nginx/">nginx</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-replication-master-master" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/05/27/replication-master-master/" class="article-date">
  <time datetime="2020-05-26T22:01:29.000Z" itemprop="datePublished">2020-05-27</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/MySQL/">MySQL</a>►<a class="article-category-link" href="/categories/MySQL/Reparation-de-replication-master-master-sans-lock/">Réparation de réplication master/master (sans lock)</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/05/27/replication-master-master/">replication-master-master</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="Reparation-de-replication-master-master-sans-lock"><a href="#Reparation-de-replication-master-master-sans-lock" class="headerlink" title="Réparation de réplication master/master sans lock"></a>Réparation de réplication master/master sans lock</h3><p><strong>Objectif</strong></p>
<p>On part du principe que notre serveur BDD-01A est OK mais que BDD-01B est <em>KO</em>.</p>
<h3 id="Actions-a-effectuer-sur-le-serveur-BDD-01B-le-KO"><a href="#Actions-a-effectuer-sur-le-serveur-BDD-01B-le-KO" class="headerlink" title="Actions à effectuer sur le serveur BDD-01B (le KO)"></a>Actions à effectuer sur le serveur BDD-01B (le KO)</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># installation des outils nécessaires</span></span><br><span class="line"></span><br><span class="line">apt-get update</span><br><span class="line">apt-get install percona-xtrabackup-24 pigz</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># On coupe la répli et on supprime les données car elles ne sont plus consistantes.  </span></span><br><span class="line"></span><br><span class="line">mysql &gt; stop slave;</span><br><span class="line">mysql &gt; reset slave;</span><br><span class="line">service mysql stop</span><br><span class="line">rm -rf /var/lib/mysql/*</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Puis on se prépare à la réception des données à jour.</span></span><br><span class="line">mkdir /var/lib/mysql</span><br><span class="line"><span class="built_in">cd</span> /var/lib/mysql</span><br><span class="line"><span class="built_in">ulimit</span> -n 65535</span><br><span class="line">nc <span class="_">-l</span> -p 4242 | gunzip | tar ixvf -</span><br></pre></td></tr></table></figure>
<ul>
<li>Ulimit pour augmenter le nombre de fichiers max dans le dossier</li>
<li>nc pour avoir un pipe réseau a travers duquel transitera les données</li>
<li>gunzip et tar pour extraire ce que nous allons compresser de l’autre coté du pipe</li>
</ul>
<h3 id="Actions-a-effectuer-sur-le-serveur-BDD-01A-le-OK"><a href="#Actions-a-effectuer-sur-le-serveur-BDD-01A-le-OK" class="headerlink" title="Actions à effectuer sur le serveur BDD-01A (le OK)"></a>Actions à effectuer sur le serveur BDD-01A (le OK)</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Installation des outils nécessaires</span></span><br><span class="line">apt-get update</span><br><span class="line">apt-get install percona-xtrabackup-24 pigz</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># On coupe la réplication</span></span><br><span class="line">mysql &gt; reset master;</span><br><span class="line">mysql &gt; stop slave;</span><br><span class="line">mysql &gt; reset slave;</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># On effectue le backup</span></span><br><span class="line">innobackupex --defaults-file=/etc/mysql/conf.d/super_custom.cnf --user=root --password=<span class="string">'ChangeMe'</span> --stream=tar ./ |  pigz | nc -q 0 bdd-replica 4242</span><br><span class="line"><span class="comment"># Adaptez le super_custom.cnf a votre conf</span></span><br><span class="line"><span class="comment"># bdd-replica doit exister dans vos /etc/hosts </span></span><br><span class="line"><span class="comment"># sur BDD-01A bdd-replica sera un alias de BDD-01B et vice versa</span></span><br><span class="line"><span class="comment"># Pensez à créer ces alias ;-)</span></span><br></pre></td></tr></table></figure>
<h3 id="Actions-a-effectuer-sur-le-serveur-BDD-01B-le-KO-1"><a href="#Actions-a-effectuer-sur-le-serveur-BDD-01B-le-KO-1" class="headerlink" title="Actions à effectuer sur le serveur BDD-01B (le KO)"></a>Actions à effectuer sur le serveur BDD-01B (le KO)</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Application des logs</span></span><br><span class="line">innobackupex --use-memory=1G --apply-log /var/lib/mysql/</span><br><span class="line"><span class="built_in">echo</span> 3 &gt; /proc/sys/vm/drop_caches</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Restart mysql</span></span><br><span class="line">chown -R mysql: /var/lib/mysql</span><br><span class="line">service mysql start</span><br></pre></td></tr></table></figure>
<p>/!\ Si et seulement si on a une erreur de connection du user debian-sys-maint lors du mysql start /!\</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Fix debian-sys-maint</span></span><br><span class="line">cat /etc/mysql/debian.cnf</span><br><span class="line">SET PASSWORD FOR <span class="string">'debian-sys-maint'</span>@<span class="string">'localhost'</span> = PASSWORD(<span class="string">'ChangeMe'</span>);</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># On affiche les informations de la réplication pour la relancer</span></span><br><span class="line">cat /var/lib/mysql/xtrabackup_binlog_info</span><br><span class="line">master-log-bin.000001   36470842</span><br></pre></td></tr></table></figure>
<p>On prends les infos de ce cat et on les reporte dans la commande qui suit :</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Remise en place de la réplication</span></span><br><span class="line">mysql &gt; change master to master_host=<span class="string">'bdd-replica'</span>, master_port=3306, master_user=<span class="string">'repl_user'</span>, master_password=<span class="string">'ChangeMe'</span>, master_log_file=<span class="string">'master-log-bin.000001'</span>, master_log_pos=36470842;</span><br><span class="line">mysql &gt; start slave;</span><br><span class="line">watch <span class="string">'mysql -e "show slave status\G" | grep Seconds_Behind_Master'</span></span><br></pre></td></tr></table></figure>
<p>On attends que la répli rattrape son retard, quand c’est ok on peut faire la suite</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Remise en place de la répli master</span></span><br><span class="line">mysql &gt; reset master;</span><br><span class="line">mysql &gt; show master status;</span><br><span class="line">    master-log-bin.000001   42</span><br></pre></td></tr></table></figure>
<h3 id="Actions-a-effectuer-sur-le-serveur-BDD-01A-le-OK-1"><a href="#Actions-a-effectuer-sur-le-serveur-BDD-01A-le-OK-1" class="headerlink" title="Actions à effectuer sur le serveur BDD-01A (le OK)"></a>Actions à effectuer sur le serveur BDD-01A (le OK)</h3><p>On prends les infos du show master status du serveur 01b comme fait précédemment et on les reporte dans la commande qui suit :</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Remise en place de la répli</span></span><br><span class="line">mysql &gt; change master to master_host=<span class="string">'bdd-replica'</span>, master_port=3306, master_user=<span class="string">'repl_user'</span>, master_password=<span class="string">'ChangeMe'</span>, master_log_file=<span class="string">'master-log-bin.000001'</span>, master_log_pos=42;</span><br><span class="line">mysql &gt; start slave;</span><br><span class="line">mysql &gt; show slave status\G</span><br></pre></td></tr></table></figure>
<p>Enjoy \o/ </p>
<p><a href="https://www.percona.com/doc/percona-xtrabackup/2.3/innobackupex/streaming_backups_innobackupex.html" target="_blank" rel="noopener">Source</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://91.208.181.214/2020/05/27/replication-master-master/" data-id="ckf6o8c19000kxsiyn0aya3q7" class="article-share-link">Partager</a>
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/mysql/">mysql</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-Ansible-quick-start" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/05/21/Ansible-quick-start/" class="article-date">
  <time datetime="2020-05-21T18:21:42.000Z" itemprop="datePublished">2020-05-21</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Ansible/">Ansible</a>►<a class="article-category-link" href="/categories/Ansible/Quick-Starts/">Quick Starts</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/05/21/Ansible-quick-start/">Ansible-quick-start</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Petit retour d’expérience conçu pour permettre de démarrer rapidement avec les éléments essentiels d’Ansible.</p>
<p>Work in progress… :watch:</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://91.208.181.214/2020/05/21/Ansible-quick-start/" data-id="ckf6o8c0b0001xsiygudy2kjt" class="article-share-link">Partager</a>
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Ansible/">Ansible</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-Elastic-Stack-Essentials" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/04/26/Elastic-Stack-Essentials/" class="article-date">
  <time datetime="2020-04-26T10:21:42.000Z" itemprop="datePublished">2020-04-26</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/ElasticSearch/">ElasticSearch</a>►<a class="article-category-link" href="/categories/ElasticSearch/Playing-with-Elastic-Stack-Modules/">Playing with Elastic Stack Modules</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/26/Elastic-Stack-Essentials/">Elastic-Stack-Essentials</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Lab effectué sur <a href="https://linuxacademy.com/" target="_blank" rel="noopener">Linuxacademy</a>.</p>
<p>Le cours <em>Elastic Stack Essentials (Legacy)</em> est une introduction sympa à la stack Elastic qui vous propose après une présentation succinte de<br>Kibana, Logstash, Beats, X-pack etc…de monter un petit cluster de 3 noeuds afin de mettre en pratique et tester ces différents outils. Cool!</p>
<p>Voici la mise en pratique de ce Lab qui va se découper de la manière suivante: </p>
<ul>
<li>Installer ES sur chaque noeuds (Debian 10)</li>
<li>Configurer le cluster (Master / data-node)</li>
<li>Génerer et deployer un certificat sur chaque noeuds</li>
<li>Génerer un mot de passe avec elasticsearch-setup-passwords</li>
<li>Installer Kibana sur le Master node et le connecter à ES</li>
<li>Deployer et configurer metricbeat sur chaque noeuds</li>
<li>Deployer et configurer filebeat sur chaque noeuds</li>
<li>Utiliser Kibana pour visualiser nos dashboards</li>
</ul>
<h3 id="Installer-ES-sur-chaque-noeuds"><a href="#Installer-ES-sur-chaque-noeuds" class="headerlink" title="Installer ES sur chaque noeuds"></a>Installer ES sur chaque noeuds</h3><p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/deb.html" target="_blank" rel="noopener">source</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># install as root</span></span><br><span class="line">sudo su -</span><br><span class="line"><span class="comment"># import GPG key</span></span><br><span class="line">wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -</span><br><span class="line"><span class="comment"># Verify that apt-transport-https is already install</span></span><br><span class="line">root@n3ird4:~<span class="comment"># apt-cache policy apt-transport-https</span></span><br><span class="line">apt-transport-https:</span><br><span class="line">  Installed: 1.4.9</span><br><span class="line">  Candidate: 1.4.9</span><br><span class="line">  Version table:</span><br><span class="line"> *** 1.4.9 500</span><br><span class="line">        500 http://deb.debian.org/debian stretch/main amd64 Packages</span><br><span class="line">        500 http://security.debian.org stretch/updates/main amd64 Packages</span><br><span class="line">        100 /var/lib/dpkg/status</span><br><span class="line"><span class="comment"># Create repo definition</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"deb https://artifacts.elastic.co/packages/7.x/apt stable main"</span> | sudo tee <span class="_">-a</span> /etc/apt/sources.list.d/elastic-7.x.list</span><br><span class="line"><span class="comment"># Install ES \o/</span></span><br><span class="line">apt-get update &amp;&amp; apt-get install elasticsearch</span><br><span class="line"><span class="comment"># ES start on boot</span></span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl <span class="built_in">enable</span> elasticsearch.service</span><br></pre></td></tr></table></figure>
<h3 id="Configurer-le-cluster-Master-data-node"><a href="#Configurer-le-cluster-Master-data-node" class="headerlink" title="Configurer le cluster (Master / data-node)"></a>Configurer le cluster (Master / data-node)</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sur les 3 noeuds il faut éditer le /etc/elasticsearch/elasticsearch.yml tel que :</span></span><br><span class="line">cluster.name: whatever42</span><br><span class="line">network.host: [_local_, _site_]</span><br><span class="line">discovery.seed_hosts: [<span class="string">"XX.XX.XX.XX"</span>] (My master-node private ip Address from Linuxacademy)</span><br><span class="line">cluster.initial_master_nodes: [<span class="string">"master-1"</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># puis on passe à la configuration des noeuds un par un :</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># sur le master</span></span><br><span class="line">node.name: master-1</span><br><span class="line">node.master: <span class="literal">true</span></span><br><span class="line">node.data: <span class="literal">false</span></span><br><span class="line">node.ingest: <span class="literal">true</span></span><br><span class="line">node.ml: <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># sur le data-1</span></span><br><span class="line">node.name: data-1</span><br><span class="line">node.master: <span class="literal">false</span></span><br><span class="line">node.data: <span class="literal">true</span></span><br><span class="line">node.ingest: <span class="literal">false</span></span><br><span class="line">node.ml: <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># sur le data-2</span></span><br><span class="line">node.name: data-2</span><br><span class="line">node.master: <span class="literal">false</span></span><br><span class="line">node.data: <span class="literal">true</span></span><br><span class="line">node.ingest: <span class="literal">false</span></span><br><span class="line">node.ml: <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># On start ES</span></span><br><span class="line"></span><br><span class="line">systemctl start elasticsearch</span><br><span class="line"></span><br><span class="line"><span class="comment"># et on check via l'API cat que tout va bien :</span></span><br><span class="line"></span><br><span class="line">root@n3ird4:~<span class="comment"># curl localhost:9200/_cat/nodes?v</span></span><br><span class="line">ip            heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name</span><br><span class="line">172.31.35.158            9          96  60    0.66    0.21     0.08 im        *      master-1</span><br><span class="line">172.31.37.11             8          96  43    0.38    0.14     0.05 d         -      data-1</span><br><span class="line">172.31.39.155            8          97  45    0.50    0.18     0.07 d         -      data-2</span><br><span class="line"></span><br><span class="line"><span class="comment"># au passage petit tip pratique pour lister les commandes dispo de l'API _cat : </span></span><br><span class="line"></span><br><span class="line">root@n3ird4:~<span class="comment"># curl http://localhost:9200/_cat/</span></span><br><span class="line">=^.^=</span><br><span class="line">/_cat/allocation</span><br><span class="line">/_cat/shards</span><br><span class="line">/_cat/shards/&#123;index&#125;</span><br><span class="line">/_cat/master</span><br><span class="line">/_cat/nodes</span><br><span class="line">/_cat/indices</span><br><span class="line">/_cat/indices/&#123;index&#125;</span><br><span class="line">/_cat/segments</span><br><span class="line">/_cat/segments/&#123;index&#125;</span><br><span class="line">/_cat/count</span><br><span class="line">/_cat/count/&#123;index&#125;</span><br><span class="line">/_cat/recovery</span><br><span class="line">/_cat/recovery/&#123;index&#125;</span><br><span class="line">/_cat/health</span><br><span class="line">/_cat/pending_tasks</span><br><span class="line">/_cat/aliases</span><br><span class="line">/_cat/aliases/&#123;<span class="built_in">alias</span>&#125;</span><br><span class="line">/_cat/thread_pool</span><br><span class="line">/_cat/plugins</span><br><span class="line">/_cat/fielddata</span><br><span class="line">/_cat/fielddata/&#123;fields&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Generer-et-deployer-un-certificat-sur-chaque-noeuds-Xpack"><a href="#Generer-et-deployer-un-certificat-sur-chaque-noeuds-Xpack" class="headerlink" title="Génerer et deployer un certificat sur chaque noeuds + Xpack"></a>Génerer et deployer un certificat sur chaque noeuds + Xpack</h3><p>Dans cette partie on apprend comment sécuriser de manière minimale un cluster ES à savoir:</p>
<ul>
<li>Générer un certificat auto-signé</li>
<li>Configurer le plugin ES Security</li>
<li>Activer le chiffrement </li>
<li>Activer l’auth des différents users</li>
<li>Définir les passwords </li>
<li>Interagir avec le cluster de manière “sécurisée” :) </li>
</ul>
<p>/!\ spoiler alert: Ici on cherche à se simplifier la vie donc on va creer un seul certif sur le master pour ensuite le copier sur les data-nodes /!\</p>
<p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/configuring-tls.html#node-certificates" target="_blank" rel="noopener">Doc ES sur la génération de certrif </a><br><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/security-settings.html#transport-tls-ssl-settings" target="_blank" rel="noopener">Doc ES sur la conf TLS/SSL</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># A faire sur chaque noeuds :</span></span><br><span class="line">mkdir /etc/elasticsearch/certs</span><br><span class="line"></span><br><span class="line"><span class="comment"># A faire uniquement depuis le master</span></span><br><span class="line">/usr/share/elasticsearch/bin/elasticsearch-certutil cert --name n3ird4 --out /etc/elasticsearch/certs/n3ird4 (on donne le nom de notre cluster.name)</span><br><span class="line"></span><br><span class="line"><span class="comment"># puis on chmod + copie la clé sur les 2 autres noeuds et basta</span></span><br><span class="line"></span><br><span class="line">chmod 640 /etc/elasticsearch/certs/n3ird4</span><br><span class="line"></span><br><span class="line">scp /etc/elasticsearch/certs/development IPdedata-1:/etc/elasticsearch/certs/</span><br><span class="line"></span><br><span class="line">scp /etc/elasticsearch/certs/development IPdedata-2:/etc/elasticsearch/certs/</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># On edite à nouveau le elasticsearch.yml sur les 3 noeuds pour activer X-Pack et ajouter:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># ---------------------------------- Security ----------------------------------</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">xpack.security.enabled: <span class="literal">true</span></span><br><span class="line">xpack.security.transport.ssl.enabled: <span class="literal">true</span></span><br><span class="line">xpack.security.transport.ssl.verification_mode: certificate</span><br><span class="line">xpack.security.transport.ssl.keystore.path: certs/n3ird4</span><br><span class="line">xpack.security.transport.ssl.truststore.path: certs/n3ird4</span><br><span class="line"></span><br><span class="line"><span class="comment">#puis on restart ES sur les 3 noeuds dans la foulée pour prendre en compte les modifications:</span></span><br><span class="line"></span><br><span class="line">systemctl restart elasticsearch</span><br></pre></td></tr></table></figure>
<h3 id="Generer-un-mot-de-passe-avec-elasticsearch-setup-passwords"><a href="#Generer-un-mot-de-passe-avec-elasticsearch-setup-passwords" class="headerlink" title="Génerer un mot de passe avec elasticsearch-setup-passwords"></a>Génerer un mot de passe avec elasticsearch-setup-passwords</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cette partie est pas super interessante je passe donc vite :</span></span><br><span class="line"><span class="comment"># A faire depuis le master</span></span><br><span class="line">/usr/share/elasticsearch/bin/elasticsearch-setup-passwords interactive</span><br></pre></td></tr></table></figure>
<h3 id="Installer-Kibana-sur-le-Master-node-et-le-connecter-a-ES-via-le-8080"><a href="#Installer-Kibana-sur-le-Master-node-et-le-connecter-a-ES-via-le-8080" class="headerlink" title="Installer Kibana sur le Master node et le connecter à ES via le 8080:"></a>Installer Kibana sur le Master node et le connecter à ES via le 8080:</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">apt-get install kibana</span><br><span class="line">systemctl <span class="built_in">enable</span> kibana</span><br><span class="line"></span><br><span class="line">vim /etc/kibana/kibana.yml</span><br><span class="line"></span><br><span class="line">server.port: 8080</span><br><span class="line">server.host: <span class="string">"10.0.1.101"</span></span><br><span class="line">elasticsearch.username: <span class="string">"kibana"</span></span><br><span class="line">elasticsearch.password: <span class="string">"changeme"</span></span><br><span class="line"></span><br><span class="line">systemctl start kibana</span><br></pre></td></tr></table></figure>
<p>Si tout se passe correctement on se connecte au kibana via http://&lt;IP_ADDRESS_OF_MASTER-1&gt;:8080<br>En utilisant le couple user/password généré au préalable (cf: ci-dessus)</p>
<h3 id="Deployer-et-configurer-metricbeat-sur-chaque-noeuds"><a href="#Deployer-et-configurer-metricbeat-sur-chaque-noeuds" class="headerlink" title="Deployer et configurer metricbeat sur chaque noeuds"></a>Deployer et configurer metricbeat sur chaque noeuds</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">apt-get install metricbeat</span><br><span class="line">systemctl <span class="built_in">enable</span> metricbeat</span><br><span class="line"></span><br><span class="line">vim /etc/metricbeat/metricbeat.yml</span><br><span class="line"></span><br><span class="line">setup.kibana:</span><br><span class="line">  host: <span class="string">"10.0.1.101:8080"</span></span><br><span class="line">output.elasticsearch:</span><br><span class="line">  hosts: [<span class="string">"10.0.1.101:9200"</span>]</span><br><span class="line">  username: <span class="string">"elastic"</span></span><br><span class="line">  password: <span class="string">"changeme"</span></span><br><span class="line"></span><br><span class="line">metricbeat setup</span><br><span class="line">systemctl start metricbeat</span><br></pre></td></tr></table></figure>
<h3 id="Deployer-et-configurer-filebeat-sur-chaque-noeuds"><a href="#Deployer-et-configurer-filebeat-sur-chaque-noeuds" class="headerlink" title="Deployer et configurer filebeat sur chaque noeuds"></a>Deployer et configurer filebeat sur chaque noeuds</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">apt-get install filebeat</span><br><span class="line">systemctl <span class="built_in">enable</span> filebeat</span><br><span class="line"></span><br><span class="line">vim /etc/filebeat/filebeat.yml</span><br><span class="line"></span><br><span class="line">setup.kibana:</span><br><span class="line">  host: <span class="string">"10.0.1.101:8080"</span></span><br><span class="line">output.elasticsearch:</span><br><span class="line">  hosts: [<span class="string">"10.0.1.101:9200"</span>]</span><br><span class="line">  username: <span class="string">"elastic"</span></span><br><span class="line">  password: <span class="string">"changeme"</span></span><br><span class="line"></span><br><span class="line">filebeat modules <span class="built_in">enable</span> system</span><br><span class="line">filebeat setup</span><br><span class="line">systemctl start filebeat</span><br></pre></td></tr></table></figure>
<h3 id="Utiliser-Kibana-pour-visualiser-nos-dashboards"><a href="#Utiliser-Kibana-pour-visualiser-nos-dashboards" class="headerlink" title="Utiliser Kibana pour visualiser nos dashboards"></a>Utiliser Kibana pour visualiser nos dashboards</h3><p>Go sur http://&lt;PUBLIC_IP_ADDRESS_OF_MASTER-1&gt;:8080<br>Puis dans la partie Dasboard =&gt; on cherche “Filebeat System” ou “Metricbeat System”.</p>
<p>Voici quelques exemples on voit qu’on a différents choix possible :</p>
<p><img src="/images/ES_metricbeatsystem.PNG" alt="metricbeat system"></p>
<p>Ici on voit via filebeat des infos intéressantes sur les requêtes malveillantes que reçoit cluster !</p>
<p><img src="/images/ES_sshlogin.PNG" alt="ssh login dash"></p>
<p>Enfin via Metricbeat on a accés à une multitude de…métriques :D </p>
<p><img src="/images/ES_metricbeatECS.PNG" alt="metricbeat ECS"></p>
<p>Enjoy !</p>
<p><img src="https://sourceforge.net/images/icon_linux.gif" alt="Wesh!" title="I&#39;m watching you t&#39;as vu!"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://91.208.181.214/2020/04/26/Elastic-Stack-Essentials/" data-id="ckf6o8c0n0004xsiyxleswr8l" class="article-share-link">Partager</a>
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/elasticsearch/">elasticsearch</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-IoWait-Ma-Tuer" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/04/12/IoWait-Ma-Tuer/" class="article-date">
  <time datetime="2020-04-12T15:05:00.000Z" itemprop="datePublished">2020-04-12</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Sysadmin-Tips-Tricks/">Sysadmin Tips & Tricks</a>►<a class="article-category-link" href="/categories/Sysadmin-Tips-Tricks/iowait/">iowait</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/12/IoWait-Ma-Tuer/">IoWait ma Tuer</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Cet article n’est pas de moi, je lui rend simplement hommage <a href="https://web.archive.org/web/20170917105833/http://blog.developpeur-neurasthenique.fr:80/auto-hebergement-iowait-ma-tuer-1-2-vmstat-mpstat-atop-pidstat.html" target="_blank" rel="noopener">source</a> </p>
<p>J’apprecie sons bon sens de l’hummour ainsi que sa logique dans la façon de procéder à l’analyse. voici donc son contenu: </p>
<p>Rien à péter, je suis passé au SSD… » (Bernard M., passionné par la fuite en avant)<br>IOwait : le mal aimé</p>
<p>« IOwait correspond au temps d’attente du système pour l’écriture ou la lecture de données. Un IOwait de 10 % signifie que 10 % de l’activité du CPU n’est pas destinée à faire des calculs mais à attendre l’exécution d’une opération de lecture/écriture. » (Wikipedia)</p>
<p>Pour afficher le pourcentage d’IOwait, c’est « vmstat » qui s’y colle :</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vmstat 1</span></span><br><span class="line"></span><br><span class="line">procs  -----------memory---------- ---swap-- -----io---- -system-- ----cpu----  </span><br><span class="line">r  b   swpd   free   buff   cache   si   so    bi    bo   <span class="keyword">in</span>   cs  us sy id wa  </span><br><span class="line">1  0   7680  32200 353264 3111032    0    0 17173 19114  752  536   0  3 72 25  </span><br><span class="line">0  1   7680  31124 353376 3114680    0    0 12722 12798  799  569   0  3 75 22  </span><br><span class="line">0  1   7680  32916 353396 3114088    0    0  4592  4689  940  552   0  2 76 22  </span><br><span class="line">0  1   7680  33208 353432 3117292    0    0 20621 20533  739  592   0  4 77 19  </span><br><span class="line">1  1   7680  32096 353448 3119244    0    0 18426 18563  771  592   0  4 77 20</span><br></pre></td></tr></table></figure>
<p>La colonne qui nous intéresse est la dernière, celle qui se nomme « wa ». Plus cette valeur (en pourcentage) est élevée, plus le processeur se tourne les pouces parce que les disques durs sont littéralement à la bourre et n’arrivent pas à suivre la cadence.</p>
<p>Notez que, jusqu’ici tout va bien : « seulement » 20% d’IOwait (mais attendez un peu la suite).<br>IOwait le retour : petite tentative de démystification</p>
<p>La plupart du temps, quand on cherche rapidement des infos sur le sujet, on tombe surtout sur des personnes plus stressées les unes que les autres : « IOwait à 100%, suis-je maudis ? », « Trop d’IOwait, mon serveur rame à mort. » ou encore « J’ai un load average de fou à cause des IOwait, que faire ? ».</p>
<p>Ceci étant, pas de panique car pour réduire les IOwait, il suffit d’appliquer une des solutions suivantes :</p>
<ul>
<li>diminuer le nombre d’accès disque (Super, t’en as d’autres comme ça ?)</li>
<li>augmenter la charge du processeur (Gné ? Keskidi ?)</li>
<li>opter pour un processeur poussif (Chérie, fais gaffe, ce type est complètement con…)</li>
</ul>
<p>Et pourtant, je vous assure que ça marche : installez cpuburn sur un serveur saturé au niveau des IO, lancez autant de burns que le processeur a de coeurs et admirez le résultat.</p>
<p>Félicitations, les IOwait sont maintenant à 0%, n’hésitez pas à demander une augmentation à votre boss…<br>(Cependant, si le CPU crame à cause de vos conneries, ça va être une autre paire de manche)</p>
<p>J’en vois certains qui froncent les sourcils et c’est bien normal : le nombre d’accès disques n’a pas baissé d’un pouce (et le processeur est à bloc), alors c’est quoi l’embrouille ?</p>
<p>IOwait, la vengeance du come back : voyage au centre de la terre du kernel</p>
<p>Maintenant que l’on sait, au moins intuitivement, que l’indicateur d’IOwait est (plus ou moins) foireux, z’allions voir pourquoi.</p>
<p>L’aventure commence, comme d’hab, avec « /proc » :</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat /proc/stat</span></span><br><span class="line"></span><br><span class="line">cpu   14471379  41901343  7624436  1631039241  4181730  8954  254946  0  0  </span><br><span class="line">cpu0  2230907   8693273   1693928  409959262   1653894  1704  69221   0  0  </span><br><span class="line">cpu1  2183699   8699859   1554726  411272346   1392114  3277  69635   0  0  </span><br><span class="line">cpu2  4914950   12282210  2201785  405373432   646329   2223  62046   0  0  </span><br><span class="line">cpu3  5141820   12226000  2173996  404434200   489391   1749  54042   0  0  </span><br><span class="line">[...]</span><br></pre></td></tr></table></figure>
<p>En fonction du noyau utilisé, le nombre de colonnes varie, « man 5 proc » est votre ami :</p>
<p><em>/proc/stat<br>kernel/system statistics. Varies with architecture. Common entries include:<br>cpu  3357 0 4313 1362393</em></p>
<p>_The amount of time, measured in units of USERi HZ (1/100ths of a second on most architectures, use sysconf(_SC__CLK__TCK_) to obtain the right value), that the system spent in user mode, user mode with low priority (nice), system mode, and the idle task, respectively. The last value should be USER_HZ_ times the second entry in the uptime pseudo-file_</p>
<p><em>In Linux 2.6 this line includes three additional columns: iowait – time waiting for I/O to complete (since 2.5.41); irq – time servicing interrupts (since 2.6.0-test4); softirq – time servicing softirqs (since 2.6.0-test4).</em></p>
<p><em>Since Linux 2.6.11, there is an eighth column, steal – stolen time, which is the time spent in other operating systems when running in a virtualized environment</em></p>
<p><em>Since Linux 2.6.24, there is a ninth column, guest, which is the time spent running a virtual CPU for guest operating systems under the control of the Linux kernel.</em></p>
<p>La nobox tournant avec une Debian Squeeze, la version du noyau est 2.6.32-41, le nombre de colonnes est donc égal à 9. Et si on veut savoir à quoi correspondent ces fameuses colonnes, suffit d’appeler Luke à la rescousse :</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 'kernel_stat.h' contains the definitions needed for doing</span></span><br><span class="line"><span class="comment"> * some kernel statistics (CPU usage, context switches ...),</span></span><br><span class="line"><span class="comment"> * used by rstatd/perfmeter</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> cpu_usage_stat &#123;  </span><br><span class="line">  <span class="keyword">cputime64_t</span> user;</span><br><span class="line">  <span class="keyword">cputime64_t</span> nice;</span><br><span class="line">  <span class="keyword">cputime64_t</span> system;</span><br><span class="line">  <span class="keyword">cputime64_t</span> softirq;</span><br><span class="line">  <span class="keyword">cputime64_t</span> irq;</span><br><span class="line">  <span class="keyword">cputime64_t</span> idle;</span><br><span class="line">  <span class="keyword">cputime64_t</span> iowait;</span><br><span class="line">  <span class="keyword">cputime64_t</span> steal;</span><br><span class="line">  <span class="keyword">cputime64_t</span> guest;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>Bon, ok, vu du noyau, z’avions les différents « états » de chaque processeur.<br>Maintenant, ce qui nous intéresse, c’est de savoir comment iowait est incrémenté.</p>
<p>Et là, même punition, Luke s’y colle à nouveau:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Account for idle time.</span></span><br><span class="line"><span class="comment"> * @cputime: the cpu time spent in idle wait</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">account_idle_time</span><span class="params">(<span class="keyword">cputime_t</span> cputime)</span>  </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">struct</span> cpu_usage_stat *cpustat = &amp;amp;kstat_this_cpu.cpustat;</span><br><span class="line">  <span class="keyword">cputime64_t</span> cputime64 = cputime_to_cputime64(cputime);</span><br><span class="line">  <span class="keyword">struct</span> rq *rq = this_rq();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (atomic_read(&amp;amp;rq-&amp;gt;nr_iowait) &amp;gt; <span class="number">0</span>)</span><br><span class="line">    cpustat-&amp;gt;iowait = cputime64_add(cpustat-&amp;gt;iowait, cputime64);</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    cpustat-&amp;gt;idle = cputime64_add(cpustat-&amp;gt;idle, cputime64);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>En français dans le texte, ça nous donne ceci : si un CPU est « idle » (il n’a rien à faire) et qu’il y a des entrées/sorties en attente, alors le compteur « iowait » est incrémenté, sinon c’est le compteur « idle » qui est incrémenté.</p>
<p>Je vous la refais en plus clair : le compteur « iowait » ne peut être incrémenté que si le CPU est « idle ».</p>
<p>Nan, toujours pas ? Bon, ok : si un CPU est utilisé à 100%, il ne peut pas être « idle » donc l’iowait est forcément nul.</p>
<p>Ce qui nous amène aux conclusions suivantes :</p>
<ul>
<li>Plus le processeur utilisé est puissant, plus les IOwait seront élevés (à charge égale)</li>
<li>Un serveur « CPU bound » aura des IOwait très faibles même si les disques sont débordés</li>
<li>En résumé, l’indicateur IOwait ne permet pas de savoir à coup sûr si les disques limitent les performances</li>
</ul>
<p>Super, nous voilà bien avancé. Z’allions donc passer aux choses sérieuses.<br>IOwait, ze ultimate return même qu’il est pas content : mpstat</p>
<p>Sous Debian, mptstat fait partie du package sysstat : « apt-get install sysstat ».</p>
<p>Si vous vous souvenez du début de l’article, vous aurez constaté que la commande « vmstat » n’indiquait que 20% d’IOwait, pas de quoi fouetter un chat pourrait-on se dire.</p>
<p>Erreur… En fait, le système copiait un dossier vers un autre (« cp -a mon-gros-dossier copie-de-gros-dossier ») et, dans ce cas de figure, ce sont bien les disques qui ralentissent la copie. (On dit alors que le système est « IO bound » ).</p>
<p>Donc, même avec un tout petit processeur (Atom D525 1.8 Ghz : 2 coeurs + hyperthreading), il est nécessaire de surveiller à la loupe chaque coeur :</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># mpstat -P ALL 5</span></span><br><span class="line"></span><br><span class="line">Linux 2.6.32-5-amd64 (www.nobox.lan)       02/29/2012      _x86_64_        (4 CPU)</span><br><span class="line"></span><br><span class="line">02:02:04 PM  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest   %idle  </span><br><span class="line">02:02:09 PM  all    0.10    0.00    2.62   21.19    0.10    0.30    0.00    0.00   75.70  </span><br><span class="line">02:02:09 PM    0    0.40    0.00    6.15   63.10    0.00    0.60    0.00    0.00   29.76  </span><br><span class="line">02:02:09 PM    1    0.00    0.00    0.20    0.98    0.20    0.00    0.00    0.00   98.63  </span><br><span class="line">02:02:09 PM    2    0.00    0.00    4.13   21.06    0.20    0.39    0.00    0.00   74.21  </span><br><span class="line">02:02:09 PM    3    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00  100.00</span><br><span class="line"></span><br><span class="line">02:02:09 PM  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest   %idle  </span><br><span class="line">02:02:14 PM  all    2.23    0.00    2.67   23.32    0.20    0.35    0.00    0.00   71.24  </span><br><span class="line">02:02:14 PM    0    0.60    0.00    6.79   66.67    0.40    1.00    0.00    0.00   24.55  </span><br><span class="line">02:02:14 PM    1    8.12    0.00    0.59    0.00    0.20    0.00    0.00    0.00   91.09  </span><br><span class="line">02:02:14 PM    2    0.20    0.00    3.36   21.15    0.20    0.59    0.00    0.00   74.51  </span><br><span class="line">02:02:14 PM    3    0.00    0.00    0.00    5.88    0.00    0.00    0.00    0.00   94.12</span><br><span class="line"></span><br><span class="line">02:02:14 PM  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest   %idle  </span><br><span class="line">02:02:19 PM  all    0.10    0.00    2.73   22.23    0.20    0.25    0.00    0.00   74.49  </span><br><span class="line">02:02:19 PM    0    0.20    0.00    7.36   63.02    0.20    0.99    0.00    0.00   28.23  </span><br><span class="line">02:02:19 PM    1    0.00    0.00    0.00    1.58    0.40    0.00    0.00    0.00   98.02  </span><br><span class="line">02:02:19 PM    2    0.20    0.00    3.74   24.21    0.20    0.20    0.00    0.00   71.46  </span><br><span class="line">02:02:19 PM    3    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00  100.00</span><br></pre></td></tr></table></figure>
<p>La seconde ligne (CPU all) de chaque bloc nous indique la moyenne des CPU (tout comme vmstat) mais on s’en contrefiche : regardez plutôt les IOwait de chaque coeur et en particulier celui du CPU 0.</p>
<p>Tout de suite c’est moins folichon : il n’y a qu’un seul coeur qui bosse vraiment(cf:1) et pas de chance, il passe plus de 60% de son temps à attendre que les IO en cours se terminent. Mais dormez tranquilles braves gens, l’IOwait moyen n’est que de 20%…</p>
<p>Formidable, tout ça c’est très joli sauf que, trop souvent, on ne sait pas à quelle sauce les disques durs sont mangés ni qui est le coupable : il est temps de passer à la chasse à l’homme.<br>IOwait, randonnée pour un tueur : iotop, atop et pidstat à la rescousse</p>
<p>Se promener dans les bois c’est bien, trouver une proie pour se faire un casse-croute aux fines herbes (et, soyons fou, avec un peu de mayo), c’est quand même mieux : « apt-get install iotop ».</p>
<p>Histoire d’avoir quelque chose à se mettre sous la prémolaire, pour changer, on va lancer une compilation du noyau (« make-kpkg –initrd kernel_image »)</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># iotop -ako</span></span><br><span class="line"></span><br><span class="line">Total DISK READ: 0.00 K/s | Total DISK WRITE: 1272.82 K/s  </span><br><span class="line">TID   PRIO USER       DISK READ  DISK WRITE  SWAPIN     IO&amp;gt;   COMMAND  </span><br><span class="line">762   be/3 root          0.00 K    340.00 K  0.00 %  2.26 %   [jbd2/sda9-8]  </span><br><span class="line">777   be/3 root          0.00 K    160.00 K  0.00 %  0.97 %   [jbd2/sda7-8]  </span><br><span class="line">782   be/3 root          0.00 K    580.00 K  0.00 %  0.93 %   [jbd2/sda8-8]  </span><br><span class="line">772   be/3 root          0.00 K     44.00 K  0.00 %  0.30 %   [jbd2/sda6-8]  </span><br><span class="line">4918  be/4 postfix       4.00 K     24.00 K  0.00 %  0.04 %   cleanup -z -t unix -u -c  </span><br><span class="line">548   be/4 root          0.00 K    172.00 K  0.00 %  0.02 %   [flush-8:0]  </span><br><span class="line">7795  be/4 bdd-mysq      4.00 K     12.00 K  0.00 %  0.02 %   mysqld --basedir=/var/jail/ --datadir=[...]  </span><br><span class="line">4919  be/4 postfix       8.00 K     52.00 K  0.00 %  0.02 %   virtual -t unix  </span><br><span class="line">7686  be/4 bdd-mysq      0.00 K     92.00 K  0.00 %  0.00 %   mysqld --basedir=/var/jail/ --datadir=[...]  </span><br><span class="line">14190 be/4 mails         0.00 K     88.00 K  0.00 %  0.00 %   imap [xxxxxxxx@xxxxxxxxxx.fr xxx.xxx.xx.xxx]  </span><br><span class="line">12084 be/4 mails         0.00 K     40.00 K  0.00 %  0.00 %   imap [xxxxxxxx@xxxxxxxxxx.fr xxx.xxx.xx.xxx]  </span><br><span class="line">7679  be/4 bdd-mysq      0.00 K      4.00 K  0.00 %  0.00 %   mysqld --basedir=/var/jail/ --datadir=[...]  </span><br><span class="line">4377  be/4 root          0.00 K     76.00 K  0.00 %  0.00 %   rsyslogd -c4  </span><br><span class="line">31337 be/4 bdd-mysq      0.00 K      0.00 K  0.00 %  0.00 %   mysqld --basedir=/var/jail/ --datadir=[...]  </span><br><span class="line">25511 be/4 opendkim      0.00 K     28.00 K  0.00 %  0.00 %   opendkim -x /etc/opendkim.conf -u [...]  </span><br><span class="line">19797 be/4 root          0.00 K     12.00 K  0.00 %  0.00 %   vlogger (error <span class="built_in">log</span>)  </span><br><span class="line">19799 be/4 root          0.00 K     32.00 K  0.00 %  0.00 %   vlogger (access <span class="built_in">log</span>)  </span><br><span class="line">7681  be/4 bdd-mysq      0.00 K     80.00 K  0.00 %  0.00 %   mysqld --basedir=/var/jail/ --datadir=[...]  </span><br><span class="line">7685  be/4 bdd-mysq      0.00 K      8.00 K  0.00 %  0.00 %   mysqld --basedir=/var/jail/ --datadir=[...]  </span><br><span class="line">7933  be/4 root          0.00 K      8.00 K  0.00 %  0.00 %   dovecot -c /etc/dovecot/dovecot.conf  </span><br><span class="line">12080 be/4 mails         0.00 K      4.00 K  0.00 %  0.00 %   imap [xxxxxg@xxxxxxxxx.fr xxx.xxx.xx.xxx]  </span><br><span class="line">24558 be/4 www-blog      0.00 K     12.00 K  0.00 %  0.00 %   php5-cgi  </span><br><span class="line">25512 be/4 opendkim      0.00 K      0.00 K  0.00 %  0.00 %   opendkim -x /etc/opendkim.conf -u [...]</span><br></pre></td></tr></table></figure>
<p>M’ouais… Je sais pas pour vous mais de mon côté, je ne vois rien d’excitant et encore moins les processus nécessaires à la compilation du noyau. Je n’ai pas creusé mais il semblerait qu’iotop n’affiche pas tous les processus effectuant des entrées/sorties. (Si quelqu’un a plus d’infos, je suis preneur !)</p>
<p>Pas grave, essayons autre chose : « apt-get install atop » :</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># atop -c -d 5</span></span><br><span class="line"></span><br><span class="line">ATOP - www                2012/02/29  07:53:17                5 seconds elapsed  </span><br><span class="line">PRC | sys   0.33s | user   5.15s | <span class="comment">#proc    210 | #zombie    0 | #exit     48 |  </span></span><br><span class="line">CPU | sys     10% | user    104% | irq       1% | idle    284% | <span class="built_in">wait</span>      1% |  </span><br><span class="line">cpu | sys      6% | user     87% | irq       0% | idle      7% | cpu002 w  0% |  </span><br><span class="line">cpu | sys      2% | user      8% | irq       0% | idle     88% | cpu001 w  1% |  </span><br><span class="line">cpu | sys      1% | user      8% | irq       0% | idle     91% | cpu000 w  0% |  </span><br><span class="line">cpu | sys      1% | user      1% | irq       0% | idle     97% | cpu003 w  0% |  </span><br><span class="line">CPL | avg1   1.09 | avg5    1.38 | avg15   1.19 | csw     1216 | intr    3907 |  </span><br><span class="line">MEM | tot    3.9G | free  664.6M | cache   1.9G | buff  652.5M | slab  252.4M |  </span><br><span class="line">SWP | tot    4.0G | free    4.0G |              | vmcom   2.2G | vmlim   5.9G |  </span><br><span class="line">DSK |         sda | busy      2% | <span class="built_in">read</span>       9 | write     19 | avio    3 ms |  </span><br><span class="line">NET | transport   | tcpi     102 | tcpo     192 | udpi       0 | udpo       0 |  </span><br><span class="line">NET | network     | ipi      260 | ipo      350 | ipfrw    158 | deliv    102 |  </span><br><span class="line">NET | eth0     0% | pcki     264 | pcko     355 | si  244 Kbps | so  652 Kbps |  </span><br><span class="line">NET | ifb0   ---- | pcki     233 | pcko     233 | si  419 Kbps | so  419 Kbps |  </span><br><span class="line">NET | ifb2   ---- | pcki     121 | pcko     121 | si  230 Kbps | so  230 Kbps |</span><br><span class="line"></span><br><span class="line">PID  DSK COMMAND-LINE                                                  1/3  </span><br><span class="line">26991  96% /usr/bin/make <span class="_">-f</span> scripts/Makefile.build obj=arch/x86/kernel  </span><br><span class="line">12719   2% rsync --server --sender -vlogDtpre.iLsf --bwlimit=48 . /home/backup/  </span><br><span class="line">777     2% jbd2/sda7-8  </span><br><span class="line">762     0% jbd2/sda9-8  </span><br><span class="line">27030   0% &amp;lt;cc1&amp;gt;  </span><br><span class="line">27044   0% &amp;lt;cc1&amp;gt;  </span><br><span class="line">27058   0% &amp;lt;cc1&amp;gt;  </span><br><span class="line">27072   0% /usr/lib/gcc/x86_64-linux-gnu/4.4.5/cc1 -quiet -nostdinc -Iinclude -  </span><br><span class="line">27036   0% &amp;lt;cc1&amp;gt;  </span><br><span class="line">27050   0% &amp;lt;cc1&amp;gt;  </span><br><span class="line">27035   0% &amp;lt;genksyms&amp;gt;  </span><br><span class="line">27064   0% &amp;lt;cc1&amp;gt;  </span><br><span class="line">27049   0% &amp;lt;genksyms&amp;gt;  </span><br><span class="line">27063   0% &amp;lt;genksyms&amp;gt;  </span><br><span class="line">27031   0% &amp;lt;as&amp;gt;  </span><br><span class="line">20630   0% atop <span class="_">-d</span> -c 5  </span><br><span class="line">27045   0% &amp;lt;as&amp;gt;  </span><br><span class="line">27059   0% &amp;lt;as&amp;gt;  </span><br><span class="line">12703   0% sshd: root@notty  </span><br><span class="line">27024   0% &amp;lt;as --gdwarf2 -Qy --64 -o arch/x86/kernel/entry_64.o /tmp/cc697jEh.&amp;gt;  </span><br><span class="line">27039   0% &amp;lt;fixdep&amp;gt;  </span><br><span class="line">27053   0% &amp;lt;fixdep&amp;gt;  </span><br><span class="line">27067   0% &amp;lt;fixdep&amp;gt;  </span><br><span class="line">21223   0% /usr/sbin/munin-node  </span><br><span class="line">27073   0% as -Qy --64 -o arch/x86/kernel/.tmp_dumpstack_64.o -  </span><br><span class="line">3721    0% imap-login</span><br></pre></td></tr></table></figure>
<p>Ha, enfin ! Notre grand gagnant de la semaine est le processus 26991. Zoomons un peu sur ce mécréant :</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pidstat -d -p 26991 5</span></span><br><span class="line"></span><br><span class="line">Linux 2.6.32-5-amd64 (www.nobox.lan)       02/29/2012      _x86_64_        (4 CPU)</span><br><span class="line"></span><br><span class="line">07:54:42 AM       PID   kB_rd/s   kB_wr/s kB_ccwr/s  Command  </span><br><span class="line">07:54:47 AM     26991      0.00    315.20    132.80  /usr/bin/make <span class="_">-f</span> scripts/Makefile.build [...]  </span><br><span class="line">07:54:52 AM     26991      0.00    197.60     44.80  /usr/bin/make <span class="_">-f</span> scripts/Makefile.build [...]  </span><br><span class="line">07:54:57 AM     26991      0.00    185.60     50.40  /usr/bin/make <span class="_">-f</span> scripts/Makefile.build [...]  </span><br><span class="line">07:55:02 AM     26991      0.00    251.20     85.60  /usr/bin/make <span class="_">-f</span> scripts/Makefile.build [...]  </span><br><span class="line">07:55:07 AM     26991      0.00    204.80     68.00  /usr/bin/make <span class="_">-f</span> scripts/Makefile.build [...]  </span><br><span class="line">07:55:12 AM     26991      0.00    109.60      8.80  /usr/bin/make <span class="_">-f</span> scripts/Makefile.build [...]  </span><br><span class="line">07:55:17 AM     26991      0.00    132.80      8.00  /usr/bin/make <span class="_">-f</span> scripts/Makefile.build [...]</span><br></pre></td></tr></table></figure>
<p>Voilà, fin de l’histoire : on apprend que « make » se promène tranquillement (dans les bois) tout en écrivant environ 200 Ko/s.</p>
<p>Ceci étant, au lieu de surveiller un seul processus, on peut aussi demander à voir l’activité globale avec « pidstat -d -l 5 » :</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pidstat -d -l 5</span></span><br><span class="line"></span><br><span class="line">08:13:26 AM       PID   kB_rd/s   kB_wr/s kB_ccwr/s  Command  </span><br><span class="line">08:13:31 AM       762      0.00     24.51      0.00  jbd2/sda9-8  </span><br><span class="line">08:13:31 AM       782      0.00      0.79      0.00  jbd2/sda8-8  </span><br><span class="line">08:13:31 AM     19222  13796.84  14118.58      0.00  cp <span class="_">-a</span> gros-backup copie-de-gros-backup</span><br><span class="line"></span><br><span class="line">08:13:31 AM       PID   kB_rd/s   kB_wr/s kB_ccwr/s  Command  </span><br><span class="line">08:13:36 AM       762      0.00     15.20      0.00  jbd2/sda9-8  </span><br><span class="line">08:13:36 AM      1038      0.00      1.60      0.00  /usr/sbin/rsyslogd -c4  </span><br><span class="line">08:13:36 AM      5780      0.00      0.80      0.00  /usr/sbin/nmbd -D  </span><br><span class="line">08:13:36 AM     16095     32.80     20.80      0.80  /usr/lib/postfix/master  </span><br><span class="line">08:13:36 AM     19222  19984.00  19984.00      0.00  cp <span class="_">-a</span> gros-backup copie-de-gros-backup</span><br><span class="line"></span><br><span class="line">08:13:36 AM       PID   kB_rd/s   kB_wr/s kB_ccwr/s  Command  </span><br><span class="line">08:13:41 AM       762      0.00     13.60      0.00  jbd2/sda9-8  </span><br><span class="line">08:13:41 AM       782      0.00      3.20      0.00  jbd2/sda8-8  </span><br><span class="line">08:13:41 AM     16095      2.40      0.00      0.00  /usr/lib/postfix/master  </span><br><span class="line">08:13:41 AM     19222   8874.40   8993.60      0.00  cp <span class="_">-a</span> gros-backup copie-de-gros-backup</span><br></pre></td></tr></table></figure>
<p>Pour ceux qui viennent juste de comprendre, oui, depuis le début du billet et mis à part la compilation du kernel, tous les exemples sont basés sur « cp -a gros-backup copie-de-gros-backup » (mais, dans la seconde partie, on va varier les plaisirs car sinon c’est pas drôle).<br>The end ?</p>
<p>Non, suite au prochain numéro : il sera question d’IOPS, d’iostat et de plus si affinités…</p>
<p>A+</p>
<p> (1)   ainsi que le CPU 2 mais, sauf erreur, il s’agit du même coeur (Hyperthreading) ↩</p>
<p>Date de prescription :     29 février 2012<br>Effets secondaires :     Auto-hébergement<br>Médecin traitant :     tranxene50  </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://91.208.181.214/2020/04/12/IoWait-Ma-Tuer/" data-id="ckf6o8c3b002nxsiy61i9dekb" class="article-share-link">Partager</a>
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/iotop/">iotop</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/iowait/">iowait</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/vmstat/">vmstat</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-swap" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/04/11/swap/" class="article-date">
  <time datetime="2020-04-11T00:14:00.000Z" itemprop="datePublished">2020-04-11</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Sysadmin-Tips-Tricks/">Sysadmin Tips & Tricks</a>►<a class="article-category-link" href="/categories/Sysadmin-Tips-Tricks/swap/">swap</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/11/swap/">Swap &amp; CO</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="Afficher-les-processus-consommateurs-de-SWAP"><a href="#Afficher-les-processus-consommateurs-de-SWAP" class="headerlink" title="Afficher les processus consommateurs de SWAP"></a>Afficher les processus consommateurs de SWAP</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> /proc/*/status ; <span class="keyword">do</span> awk <span class="string">'/VmSwap|Name/&#123;printf $2 " " $3&#125;END&#123; print ""&#125;'</span> <span class="variable">$file</span>; <span class="keyword">done</span> | sort -k 2 -n -r | head -10</span><br></pre></td></tr></table></figure>
<h3 id="Clean-swap-area-after-using-a-memory-hogging-application"><a href="#Clean-swap-area-after-using-a-memory-hogging-application" class="headerlink" title="Clean swap area after using a memory hogging application"></a>Clean swap area after using a memory hogging application</h3><p><em>When you run a memory intensive application (large java application, etc) swap area is used as soon as memory becomes insufficient. After you close the program, the data in swap is not put back on memory and that decreases the responsiveness. Swapoff disables the swap area and forces system to put swap data be placed in memory. Since running without a swap area might be detrimental, swapon should be used to activate swap again. Both swapoff and swapon require root privileges.</em></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">swapoff <span class="_">-a</span> &amp;&amp; swapon <span class="_">-a</span></span><br></pre></td></tr></table></figure>
<h3 id="Change-your-swappiness-Ratio-under-linux"><a href="#Change-your-swappiness-Ratio-under-linux" class="headerlink" title="Change your swappiness Ratio under linux"></a>Change your swappiness Ratio under linux</h3><p><em>This command allow you to set the swappiness var at XX (default is 60). The value interval must be set between 0 and 100. If swappiness is high=Sw+ap usage is high, if swappiness is low=Ram usage is high.</em></p>
<p>check via un sysctl -a si le vm.swappiness = 0 cela signifie que l’instance ne doit pas swap tant qu’il reste de le RAM de libre !</p>
<p>Sinon y a toujours la bonne vielle methode du swapfile ^^</p>
<p><a href="https://www.commandlinefu.com/commands/matching/swap/c3dhcA==/sort-by-votes" target="_blank" rel="noopener">Sources</a></p>
<p><img src="https://sourceforge.net/images/icon_linux.gif" alt="I&#39;m watching you!" title="I&#39;m watching you!"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://91.208.181.214/2020/04/11/swap/" data-id="ckf6o8c1k000rxsiymgoabys5" class="article-share-link">Partager</a>
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/swap/">swap</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-October-2019-inspiration" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/10/20/October-2019-inspiration/" class="article-date">
  <time datetime="2019-10-20T20:12:00.000Z" itemprop="datePublished">2019-10-20</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Hexo/">Hexo</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/10/20/October-2019-inspiration/">October 2019 inspiration</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>One day I will work on this website….</p>
<p><strong>Back in the game xD</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sleep(42);</span><br></pre></td></tr></table></figure>
<p>Sleeping is not a crime.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://91.208.181.214/2019/10/20/October-2019-inspiration/" data-id="ckf6o8c0t0006xsiy83qamqpf" class="article-share-link">Partager</a>
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Blog/">Blog</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Test/">Test</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-2019-post" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/10/19/2019-post/" class="article-date">
  <time datetime="2019-10-19T20:42:42.000Z" itemprop="datePublished">2019-10-19</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/others/">others</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/10/19/2019-post/">2019_post</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Oh-Yeah"><a href="#Oh-Yeah" class="headerlink" title="Oh Yeah !!!"></a>Oh Yeah !!!</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">21*2 = 42 !</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://91.208.181.214/2019/10/19/2019-post/" data-id="ckf6o8bzm0000xsiyc3h240m7" class="article-share-link">Partager</a>
      
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/42/">42</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Univers/">Univers</a></li></ul>

    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">Suivant &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Catégories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/AWS/">AWS</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/AWS/AWS-Certified-Solutions-Architect-Associate-SAA-C02/">AWS Certified Solutions Architect Associate SAA-C02</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Ansible/">Ansible</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Ansible/Quick-Starts/">Quick Starts</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/ElasticSearch/">ElasticSearch</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/ElasticSearch/Playing-with-Elastic-Stack-Modules/">Playing with Elastic Stack Modules</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hexo/">Hexo</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Hexo/How-to/">How to</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/MySQL/">MySQL</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/MySQL/Reparation-de-replication-master-master-sans-lock/">Réparation de réplication master/master (sans lock)</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Nginx/">Nginx</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Nginx/Playing-with-Nginx-and-Dynamic-Modules/">Playing with Nginx and Dynamic Modules</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Sysadmin-Tips-Tricks/">Sysadmin Tips & Tricks</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Sysadmin-Tips-Tricks/iowait/">iowait</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Sysadmin-Tips-Tricks/swap/">swap</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Varnish/">Varnish</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Varnish/blocklist-IP-address-with-Varnish/">blocklist IP address with Varnish</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/others/">others</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Mot-clés</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/42/">42</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/AWS/">AWS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Ansible/">Ansible</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Blog/">Blog</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ModSecurity/">ModSecurity</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Test/">Test</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Univers/">Univers</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/elasticsearch/">elasticsearch</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/githooks/">githooks</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/grep/">grep</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hexo/">hexo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/iotop/">iotop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/iowait/">iowait</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mysql/">mysql</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nginx/">nginx</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sort/">sort</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/swap/">swap</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/varnish/">varnish</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/vmstat/">vmstat</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Nuage de mot-clés</h3>
    <div class="widget tagcloud">
      <a href="/tags/42/" style="font-size: 10px;">42</a> <a href="/tags/AWS/" style="font-size: 10px;">AWS</a> <a href="/tags/Ansible/" style="font-size: 10px;">Ansible</a> <a href="/tags/Blog/" style="font-size: 20px;">Blog</a> <a href="/tags/ModSecurity/" style="font-size: 10px;">ModSecurity</a> <a href="/tags/Test/" style="font-size: 20px;">Test</a> <a href="/tags/Univers/" style="font-size: 10px;">Univers</a> <a href="/tags/elasticsearch/" style="font-size: 10px;">elasticsearch</a> <a href="/tags/githooks/" style="font-size: 10px;">githooks</a> <a href="/tags/grep/" style="font-size: 10px;">grep</a> <a href="/tags/hexo/" style="font-size: 10px;">hexo</a> <a href="/tags/iotop/" style="font-size: 10px;">iotop</a> <a href="/tags/iowait/" style="font-size: 10px;">iowait</a> <a href="/tags/mysql/" style="font-size: 10px;">mysql</a> <a href="/tags/nginx/" style="font-size: 10px;">nginx</a> <a href="/tags/sort/" style="font-size: 10px;">sort</a> <a href="/tags/swap/" style="font-size: 10px;">swap</a> <a href="/tags/varnish/" style="font-size: 10px;">varnish</a> <a href="/tags/vmstat/" style="font-size: 10px;">vmstat</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">septembre 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">juillet 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">mai 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">avril 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">octobre 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">décembre 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">avril 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">mars 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Articles récents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/09/08/AWS-SAAC-02/">AWS-SAAC-02</a>
          </li>
        
          <li>
            <a href="/2020/09/01/blocklist-IP-address-with-Varnish/">blocklist IP address with Varnish</a>
          </li>
        
          <li>
            <a href="/2020/07/22/Nginx-ModSecurity/">Nginx-ModSecurity</a>
          </li>
        
          <li>
            <a href="/2020/05/27/replication-master-master/">replication-master-master</a>
          </li>
        
          <li>
            <a href="/2020/05/21/Ansible-quick-start/">Ansible-quick-start</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 n3ird4<br>
      Propulsé par <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>




  </div>
</body>
</html>